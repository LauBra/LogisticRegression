[
  {
    "objectID": "tidymodels_sklearn_overview.html",
    "href": "tidymodels_sklearn_overview.html",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#why-frameworks-like-tidymodels-and-scikit-learn",
    "href": "tidymodels_sklearn_overview.html#why-frameworks-like-tidymodels-and-scikit-learn",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "1. Why frameworks like tidymodels and scikit-learn?",
    "text": "1. Why frameworks like tidymodels and scikit-learn?\nIn both R and Python, there are many ways to fit models:\n\nR: lm(), glm(), rpart(), randomForest(), xgboost(), …\nPython: statsmodels, custom NumPy/SciPy code, or many different classes in sklearn\n\n\nEach function or class often has its own syntax, argument names and prediction methods.\nThis becomes difficult when we want to:\n\nswap one model for another,\ncompare models fairly,\nbuild end-to-end pipelines with preprocessing + model + evaluation,\ndo hyperparameter tuning cleanly and without data leakage.\n\nTwo ecosystems help us solve this:\n\n\ntidymodels in R\n\n\nscikit-learn in Python\n\nBoth provide a unified, consistent interface for models, preprocessing, pipelines and tuning.",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#tidymodels-r-model-specification-engine",
    "href": "tidymodels_sklearn_overview.html#tidymodels-r-model-specification-engine",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "2. Tidymodels (R): model specification + engine\n",
    "text": "2. Tidymodels (R): model specification + engine\n\n\n\n\n\n\n\nNote\n\n\n\nDocs:\n- Tidymodels homepage: https://www.tidymodels.org/\n- Getting started: https://www.tidymodels.org/start/\n- parsnip reference: https://parsnip.tidymodels.org/reference/index.html\n\n\nIn tidymodels, we separate:\n\n\nWhat kind of model we want → the specification\n\n\nHow it is implemented → the engine\n\n\n2.1 Model specification\nWe declare high-level models like this (classification / regression only, no engine yet):\n\n#R\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlog_spec   &lt;- logistic_reg(mode = \"classification\")\ntree_spec  &lt;- decision_tree(mode = \"classification\")\nrf_spec    &lt;- rand_forest(mode = \"regression\")\nknn_spec   &lt;- nearest_neighbor(mode = \"classification\")\n\nlog_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\nThis does not fit a model yet. It just says:\n\n“I want a logistic regression classifier”\n“I want a decision tree for classification”\netc.\n2.2 Engines: how the model is actually fit\nThe engine is the backend algorithm. We choose it with set_engine():\n\n#R\nlog_spec_glm &lt;- logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")      # uses base R glm() with binomial family\n\nlog_spec_glmnet &lt;- logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glmnet\")   # uses penalised regression (elastic net)\n\ntree_spec_rpart &lt;- decision_tree(mode = \"classification\") %&gt;%\n  set_engine(\"rpart\")    # uses rpart (CART)\n\nboost_spec_xgb &lt;- boost_tree(mode = \"classification\") %&gt;%\n  set_engine(\"xgboost\")  # uses xgboost library\n\nDifferent engines = different algorithms for the same kind of model.\n2.3 Fitting a logistic regression model with tidymodels\nBelow we use the Pima Indians diabetes data and compare to the glm() style you already know.\n\n#R\nlibrary(mlbench)\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  dplyr::select(glucose, mass, age, diabetes) %&gt;%\n  # we want \"pos\" (diabetes present) as the event of interest\n  dplyr::mutate(diabetes = forcats::fct_relevel(diabetes, \"pos\")) # here we are saying that what we are trying to predict (i=our positive, 1 class is \"pos\")\n\nsplit &lt;- initial_split(pima, prop = 0.8, strata = diabetes)\npima_train &lt;- training(split)\npima_test  &lt;- testing(split)\n\n# Model spec + engine\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")   # internally: glm(..., family = binomial)\n\nFit the model using fit() with a formula interface:\n\n#R\nlogistic_fit &lt;-\n  logistic_spec %&gt;%\n  fit(diabetes ~ glucose, data = pima_train)\n\nlogistic_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetes ~ glucose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      glucose  \n    5.13837     -0.03615  \n\nDegrees of Freedom: 613 Total (i.e. Null);  612 Residual\nNull Deviance:      793.9 \nResidual Deviance: 658.5    AIC: 662.5\n\n\nInspect the coefficients (these are log-odds / logit coefficients):\n\n#R\ntidy(logistic_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   5.14     0.464        11.1 1.53e-28\n2 glucose      -0.0361   0.00358     -10.1 6.23e-24\n\n\n2.4 Making predictions in a consistent way\nAll parsnip models support the same predict() interface:\n\n#R\npima_preds &lt;-\n  predict(logistic_fit, new_data = pima_test, type = \"prob\") %&gt;%\n  dplyr::bind_cols(pima_test)\n\nhead(pima_preds)\n\n# A tibble: 6 × 6\n  .pred_pos .pred_neg glucose  mass   age diabetes\n      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1    0.814      0.186     183  23.3    32 pos     \n2    0.0896     0.910      78  31      26 pos     \n3    0.273      0.727     115  35.3    29 neg     \n4    0.845      0.155     189  30.1    59 pos     \n5    0.295      0.705     118  45.8    31 pos     \n6    0.358      0.642     126  39.3    27 neg     \n\n\nWe can visualise predicted probability vs. glucose:\n\n#R\nlibrary(ggplot2)\n\nggplot(pima_preds, aes(x = glucose, y = .pred_pos)) +\n  geom_point(alpha = 0.4) +\n  labs(\n    x = \"Glucose\",\n    y = \"Predicted probability of diabetes (pos)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nHere:\n\n\n.pred_pos = predicted ( P( = ) )\n\n.pred_neg (not shown here) = predicted ( P() )\n2.5 Recipes + workflows (pipelines in tidymodels)\nTidymodels also includes tools for preprocessing and pipelines:\n\n\nrecipes → scaling, imputation, one-hot encoding, etc.\n\n\nworkflows → combine a model + recipe into a single object\n\n\n#R\nlog_recipe &lt;-\n  recipe(diabetes ~ glucose + age + mass, data = pima) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nlog_workflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_spec) %&gt;%\n  add_recipe(log_recipe)\n\nlog_fit_multi &lt;- fit(log_workflow, data = pima)\nlog_fit_multi\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      glucose          age         mass  \n     0.8506      -1.0395      -0.3547      -0.6433  \n\nDegrees of Freedom: 767 Total (i.e. Null);  764 Residual\nNull Deviance:      993.5 \nResidual Deviance: 755.7    AIC: 763.7\n\n\nNotice, when we open in our environment the model created, it includes too, the preprocessing information!\nThis is similar in spirit to scikit-learn pipelines.",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#scikit-learn-python-unified-estimator-api",
    "href": "tidymodels_sklearn_overview.html#scikit-learn-python-unified-estimator-api",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "3. Scikit-learn (Python): unified estimator API",
    "text": "3. Scikit-learn (Python): unified estimator API\n\n\n\n\n\n\nNote\n\n\n\nDocs:\n- Scikit-learn homepage: https://scikit-learn.org/stable/\n- User guide: https://scikit-learn.org/stable/user_guide.html\n- API overview: https://scikit-learn.org/stable/developers/develop.html#apis-of-scikit-learn-objects\n\n\nIn scikit-learn, all models follow the same estimator API:\nmodel = SomeEstimator(hyperparameters)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nExamples:\n\n\nLinearRegression (ordinary least squares)\n\nLogisticRegression (classification)\nDecisionTreeClassifier\nRandomForestClassifier\n\nSVC (support vector classifier)\nKNeighborsClassifier\n\nMLPClassifier (neural network)\nPreprocessors like StandardScaler, OneHotEncoder\n\n\n3.1 A simple logistic regression in scikit-learn\n\n#Python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ndata = load_breast_cancer()\nX = data.data\ny = data.target   # 0 = malignant, 1 = benign\n\n# Train / test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Define model (specification)\nlog_reg = LogisticRegression(max_iter=1000,\n    solver=\"liblinear\")\n\n# Fit model\nlog_reg.fit(X_train, y_train)\n\n\n\n\nLogisticRegression(max_iter=1000, solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n# Predict\ny_pred = log_reg.predict(X_test)\n\nHere:\n\n\nLogisticRegression(...) = model specification (similar to logistic_reg() in parsnip)\nUnder the hood, it calls a numerical solver (e.g. \"lbfgs\", \"liblinear\", \"saga\") – this is analogous to an engine.\n3.2 Probabilities and log-odds (logits)\nWe can access predicted probabilities with predict_proba:\n\n#Python\ny_proba = log_reg.predict_proba(X_test)\ny_proba[:5]\n\narray([[7.09013048e-01, 2.90986952e-01],\n       [1.07801108e-02, 9.89219889e-01],\n       [6.67983197e-04, 9.99332017e-01],\n       [9.89039021e-01, 1.09609792e-02],\n       [2.77066904e-01, 7.22933096e-01]])\n\n\n\nColumn 0 = (P( x))\n\nColumn 1 = (P( x))\n\nThe underlying logits (log-odds) are related to these probabilities via the logistic function.",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#pipelines-tidymodels-workflows-vs-scikit-learn-pipeline",
    "href": "tidymodels_sklearn_overview.html#pipelines-tidymodels-workflows-vs-scikit-learn-pipeline",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "4. Pipelines: tidymodels workflows vs scikit-learn Pipeline",
    "text": "4. Pipelines: tidymodels workflows vs scikit-learn Pipeline\nBoth ecosystems support pipelines to chain preprocessing + model in a leak-free way.\n4.1 Tidymodels workflow (R)\nWe already saw:\n\n#R\nlog_recipe &lt;-\n  recipe(diabetes ~ glucose + age + mass, data = pima) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")\n\nlog_workflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_spec) %&gt;%\n  add_recipe(log_recipe)\n\nlog_fit_multi &lt;- fit(log_workflow, data = pima)\n\nHere:\n\n\nrecipe = preprocessing steps\n\n\nlogistic_spec = model specification + engine\n\n\nworkflow = pipeline object (prep + fit together)\n4.2 Scikit-learn Pipeline (Python)\nThe scikit-learn equivalent uses Pipeline and transformers:\n\n#Python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),          # preprocessing step\n    (\"log_reg\", LogisticRegression(\n    max_iter=1000,\n    solver=\"liblinear\"\n))\n])\n\npipe.fit(X_train, y_train)\n\n\n\n\nPipeline(steps=[('scale', StandardScaler()),\n                ('log_reg',\n                 LogisticRegression(max_iter=1000, solver='liblinear'))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipeline\n\n?Documentation for PipelineiFitted\n\n        \n            Parameters\n\n\nsteps \n[('scale', ...), ('log_reg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n    \n\n\n\nStandardScaler\n?Documentation for StandardScaler\n        \n            Parameters\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n    \n\n\nLogisticRegression\n?Documentation for LogisticRegression\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n\npipe.score(X_test, y_test)\n\n0.9883040935672515\n\n\n\n\nStandardScaler() = transformer (similar to step_normalize)\n\n\nLogisticRegression() = estimator (similar role to logistic_reg() + engine \"glm\")\n\n\nPipeline = links them, ensures the same transformations are applied in training and testing.",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#summary-parsnip-tidymodels-vs-scikit-learn",
    "href": "tidymodels_sklearn_overview.html#summary-parsnip-tidymodels-vs-scikit-learn",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "6. Summary: parsnip + tidymodels vs scikit-learn",
    "text": "6. Summary: parsnip + tidymodels vs scikit-learn\nBoth ecosystems share the same design philosophy:\n\n\nUnified interface for all models\n\nR: fit(), predict() on parsnip/workflow objects\n\nPython: fit(), predict() on estimators\n\n\n\nPipelines and tuning are built-in\n\nLeak-free preprocessing\n\n\n\nThis allows you to focus on the modelling ideas (classification, regression, evaluation, bias–variance, etc.) instead of constantly fighting with different syntaxes and one-off implementations.",
    "crumbs": [
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "logistic_regression_python.html",
    "href": "logistic_regression_python.html",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))",
    "crumbs": [
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#data-pima-indians-diabetes",
    "href": "logistic_regression_python.html#data-pima-indians-diabetes",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Data: Pima Indians Diabetes",
    "text": "Data: Pima Indians Diabetes\n\nimport pandas as pd\n\n\nurl=\"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\npima= pd.read_csv(url)\n\npima = pima[[\"Glucose\", \"Outcome\", \"BloodPressure\", \"Age\"]]\npima[\"Diabetes_binary\"] = (pima[\"Outcome\"] == 1).astype(int)\npima.head()\n\n   Glucose  Outcome  BloodPressure  Age  Diabetes_binary\n0      148        1             72   50                1\n1       85        0             66   31                0\n2      183        1             64   32                1\n3       89        0             66   21                0\n4      137        1             40   33                1",
    "crumbs": [
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#logistic-regression-with-scikit-learn",
    "href": "logistic_regression_python.html#logistic-regression-with-scikit-learn",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Logistic regression with scikit-learn",
    "text": "Logistic regression with scikit-learn\n\nfrom sklearn.linear_model import LogisticRegression\n\nX = pima[[\"Glucose\"]].values\ny = pima[\"Diabetes_binary\"].values\n\nlog_reg = LogisticRegression(solver=\"lbfgs\")\nlog_reg.fit(X, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\nlog_reg.intercept_, log_reg.coef_\n\n(array([-5.34990586]), array([[0.03787169]]))",
    "crumbs": [
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#predictions-plots",
    "href": "logistic_regression_python.html#predictions-plots",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Predictions & plots",
    "text": "Predictions & plots\n\nprobs = log_reg.predict_proba(X)\npima[\"prob_diabetes_1\"] = probs[:, 1]\npima[\"prob_diabetes_0\"] = probs[:, 0]\npima.head()\n\n   Glucose  Outcome  ...  prob_diabetes_1  prob_diabetes_0\n0      148        1  ...         0.563432         0.436568\n1       85        0  ...         0.106138         0.893862\n2      183        1  ...         0.829291         0.170709\n3       89        0  ...         0.121391         0.878609\n4      137        1  ...         0.459716         0.540284\n\n[5 rows x 7 columns]\n\n\nWhich probability column do we want?",
    "crumbs": [
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "Gradients.html",
    "href": "Gradients.html",
    "title": "3 Gradients",
    "section": "",
    "text": "How are we finding the model that best fits the data? Through optimization! Here we calculate the parameters B0 and B1 using gradient descent of log-loss, gradient ascent of log likelihood and compare it with the function glm that already does this for us.\n\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Load the Pima Indians Diabetes dataset\ndata(\"PimaIndiansDiabetes\")\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes) %&gt;%\n  mutate(diabetes_dummy = ifelse(diabetes == \"pos\", 1, 0))\n\n\n# Data preparation\nx &lt;- PimaIndiansDiabetes$glucose\ny &lt;- PimaIndiansDiabetes$diabetes_dummy\n\n\n# Sigmoid function\nsigmoid &lt;- function(z) {\n  1 / (1 + exp(-z))\n}\n\n\n# Log-loss function (for gradient descent) - coded equation from slides\nlog_loss &lt;- function(beta_0, beta_1, x, y) {\n  z &lt;- beta_0 + beta_1 * x\n  y_pred &lt;- sigmoid(z)\n  -sum(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n}\n\n\n# Log-likelihood function (for gradient ascent)\nlog_likelihood &lt;- function(beta_0, beta_1, x, y) {\n  z &lt;- beta_0 + beta_1 * x\n  y_pred &lt;- sigmoid(z)\n  sum(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n}\n\nTo plot - do not need to know this!\n\n# Generate data\nset.seed(123)\n#x &lt;- seq(50, 200, length.out = 100)  # glucose values\n#y_dummy &lt;- sample(0:1, length(x), replace = TRUE)  # random dummy diabetes values\n\n# Generate a grid of beta_0 and beta_1 values\nbeta_0_values &lt;- seq(-10, 10, length.out = 50)\nbeta_1_values &lt;- seq(-1, 1, length.out = 50)\n\n# Compute log-loss and log-likelihood values\nlog_loss_matrix &lt;- outer(beta_0_values, beta_1_values, Vectorize(function(b0, b1) log_loss(b0, b1, x, y)))\nlog_likelihood_matrix &lt;- outer(beta_0_values, beta_1_values, Vectorize(function(b0, b1) log_likelihood(b0, b1, x, y)))\n\n# Plot the log-loss function\nlog_loss_plot &lt;- plot_ly(\n  x = beta_0_values, \n  y = beta_1_values, \n  z = log_loss_matrix, \n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"Log-Loss Function\",\n    scene = list(\n      xaxis = list(title = \"Beta 0\"),\n      yaxis = list(title = \"Beta 1\"),\n      zaxis = list(title = \"Log-Loss\")\n    )\n)\n\n# Plot the log-likelihood function\nlog_likelihood_plot &lt;- plot_ly(\n  x = beta_0_values, \n  y = beta_1_values, \n  z = log_likelihood_matrix, \n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"Log-Likelihood Function\",\n    scene = list(\n      xaxis = list(title = \"Beta 0\"),\n      yaxis = list(title = \"Beta 1\"),\n      zaxis = list(title = \"Log-Likelihood\")\n    )\n)\n\n# Display the plots\nlog_loss_plot\n\n\n\n\nlog_likelihood_plot\n\n\n\n\n\n\n# Gradient descent implementation\ngradient_descent &lt;- function(x, y, learning_rate, iterations) { # we can play with these two!\n  store_beta_0 &lt;- NULL\n  store_beta_1 &lt;- NULL\n  beta_0 &lt;- -5  # Initialize intercept #random points! Can play around with these ones too\n  beta_1 &lt;- 1  # Initialize slope #random points! Can play around with these ones too\n  for (i in 1:iterations) {\n    z &lt;- beta_0 + beta_1 * x #linear regression function/logit\n    y_pred &lt;- sigmoid(z) #logistic regression\n    grad_beta_0 &lt;- mean(y_pred - y) #partial derivative of intercept\n    grad_beta_1 &lt;- mean((y_pred - y) * x) #partial derivative of slope\n    beta_0 &lt;- beta_0 - learning_rate * grad_beta_0 #gradient descent equation! Updating the points\n    beta_1 &lt;- beta_1 - learning_rate * grad_beta_1\n    store_beta_0[i] &lt;- beta_0\n    store_beta_1[i] &lt;- beta_1\n  }\n  data.frame(beta_1 = store_beta_1, beta_0 = store_beta_0)\n}\n\n\ngd_result &lt;- gradient_descent(x, y, learning_rate = 0.01, iterations = 10)\n#View(gd_result)\n\nMinimum point found after 1000 iterations:\n\npaste0(\"Glucose/Slope:, \", gd_result$beta_1[dim(gd_result)[2]])\n\n[1] \"Glucose/Slope:, -0.432030882563657\"\n\npaste0(\"Intercept: \", gd_result$beta_0[dim(gd_result)[2]])\n\n[1] \"Intercept: -5.01289148560383\"\n\n\n\n# Gradient ascent implementation\ngradient_ascent &lt;- function(x, y, learning_rate, iterations) {\n  store_beta_0 &lt;- NULL\n  store_beta_1 &lt;- NULL\n  beta_0 &lt;- 0  # Initialize intercept\n  beta_1 &lt;- 0  # Initialize slope\n  for (i in 1:iterations) {\n    z &lt;- beta_0 + beta_1 * x\n    y_pred &lt;- sigmoid(z)\n    grad_beta_0 &lt;- mean(y - y_pred)\n    grad_beta_1 &lt;- mean((y - y_pred) * x)\n    beta_0 &lt;- beta_0 + learning_rate * grad_beta_0\n    beta_1 &lt;- beta_1 + learning_rate * grad_beta_1\n    store_beta_0[i] &lt;- beta_0\n    store_beta_1[i] &lt;- beta_1\n  }\n  data.frame(beta_1 = store_beta_1, beta_0 = store_beta_0)\n}\n\nWhat do you notice as difference? The sign!\n\nga_result &lt;- gradient_ascent(x, y, learning_rate = 0.01, iterations = 10)\nView(ga_result)\n\n\npaste0(\"Glucose/Slope:, \", ga_result$beta_1[dim(ga_result)[2]])\n\n[1] \"Glucose/Slope:, 0.381357711035091\"\n\npaste0(\"Intercept: \", ga_result$beta_0[dim(ga_result)[2]])\n\n[1] \"Intercept: 0.00194621080983146\"\n\n\nIf we fit the logistic regression model as before we get:\n\n# Fit a logistic regression model using glm\n\ndataset &lt;- data.frame(x =x, y = y)  # random dummy diabetes values\n\nglm_model &lt;- glm(y ~ x, data = dataset, family = binomial)\ncoef(glm_model)\n\n(Intercept)           x \n-5.35008039  0.03787304 \n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "DecisionThreshold.html",
    "href": "DecisionThreshold.html",
    "title": "2 Decision threshold",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# Load the Pima Indians Diabetes dataset\ndata(\"PimaIndiansDiabetes\")\n\n\n# Narrow down the dataset\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes)\n\n# Convert the outcome variable to binary (0 = neg, 1 = pos)\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(diabetes_dummy = ifelse(diabetes == \"pos\", 1, 0))\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# Predict probabilities\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(predicted_probability = predict(logistic_model, type = \"response\"))\n\n\n# Create classifications for different thresholds\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(\n    class_0_5 = ifelse(predicted_probability &gt;= 0.5, 1, 0),\n    class_0_3 = ifelse(predicted_probability &gt;= 0.3, 1, 0),\n    class_0_7 = ifelse(predicted_probability &gt;= 0.7, 1, 0)\n  )\n\nWhat is hapenning when we increase the threshold? See how the number of correct (true y and predicted y) change!\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_3)\n\n   \n      0   1\n  0 331 169\n  1  66 202\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_5)\n\n   \n      0   1\n  0 443  57\n  1 138 130\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_7)\n\n   \n      0   1\n  0 484  16\n  1 195  73\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Basic_Understanding2.html",
    "href": "Basic_Understanding2.html",
    "title": "(1a) Fitting a logistic regression model - R",
    "section": "",
    "text": "First lets start with setting probabilities, odds and log odds and seeing how they are all related:\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.1f\\n\", prob))\n\nProbability: 0.8\n\n#Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.1f\\n\", odds))\n\nOdds: 4.0\n\n#Convert odds to log-odds\nlogodds &lt;- log(odds)\ncat(sprintf(\"Log-Odds: %.1f\\n\", logodds))\n\nLog-Odds: 1.4\n\n\nNow, lets fit a logistic regression model. As we said yesterday, the learning algorithm is already coded for us! So we just have to apply the correct function. In this case we will use glm() and set as input parameters family = binomial https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm, https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family which specifies implementing a logistic regression algorithm by setting the link function to logit . We could have also used any of the models in tidymodels https://parsnip.tidymodels.org/reference/logistic_reg.html\nThe data we will be using is the same as yesterday’s, PimaIndiansDiabetes\n\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n\nWe have lots of features/columns, so lets narrow it down to just a few\n\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes, mass, age)\n\n\nOutcome: “diabetes” is the target variable.Its a categorical varaible, made up of only two values, neg and pos\n\n\n\ntable(PimaIndiansDiabetes$diabetes)\n\n\nneg pos \n500 268 \n\n\nFor our first logistic regression model, we are going to just use glucose as predictor only. We can specify this in the following way:\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n\n\nCall:\nglm(formula = diabetes ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.350080   0.420827  -12.71   &lt;2e-16 ***\nglucose      0.037873   0.003252   11.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n\n\nWhat is the predictive model we have fitted?\nThis one!\n\\[\nP = \\frac{1}{1 + \\exp\\left(-\\left(-5.35 + 0.0379 \\cdot \\text{glucose}\\right)\\right)}\n\\] Where glm() has done all teh work for us, and identified the parameters \\(B_0\\) and \\(B_1\\) that best fit our dataset.These parameters are:\n\nintercept &lt;- coef(logistic_model)[1]\nslope &lt;- coef(logistic_model)[2]\n\nintercept\n\n(Intercept) \n   -5.35008 \n\nslope\n\n   glucose \n0.03787304 \n\n\nWith this information, we could calcluate for example, the odds of having diabetes, if \\(glucose = 120\\)\n\n#odds transformation\nodds &lt;- exp(intercept + slope * 120) # Example: glucose = 120\ncat(\"Odds at glucose = 120:\", odds, \"\\n\")\n\nOdds at glucose = 120: 0.4469466 \n\n\nJust as a side not, we want pos to be our predicted outcome, shpuld we ‘dummify’ the column so we have 0s and 1s only? Lets try and see:\n\nPimaIndiansDiabetes$diabetes_dummy &lt;- ifelse(PimaIndiansDiabetes$diabetes ==  \"pos\", 1, 0)\nhead(PimaIndiansDiabetes$diabetes_dummy)\n\n[1] 1 0 1 0 1 0\n\n\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n\n\nCall:\nglm(formula = diabetes_dummy ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.350080   0.420827  -12.71   &lt;2e-16 ***\nglucose      0.037873   0.003252   11.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe get the same results! glm recognizes this as a category and does it for us!\nNow, lets make predictions. We are going to use our same dataset we used to fit our data, and see how much the predicted probabilities differ from the real/true classes. Again, we use the function predict.\n\npredictions &lt;- predict(logistic_model, type = \"response\")\n\nLets add them to the dataset using tidyverse\n\n# Add predictions to the dataset\nPimaIndiansDiabetesWithPred &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(predicted_probability = predictions)\n\n# Preview data with predictions\nhead(PimaIndiansDiabetesWithPred)\n\n  glucose diabetes mass age diabetes_dummy predicted_probability\n1     148      pos 33.6  50              1             0.5634385\n2      85      neg 26.6  31              0             0.1061322\n3     183      pos 23.3  32              1             0.8293015\n4      89      neg 28.1  21              0             0.1213849\n5     137      pos 43.1  33              1             0.4597189\n6     116      neg 25.6  30              0             0.2775177\n\n\nWe do see a pattern right? Those with diabetes = pos (our 1) have higher probabilities. Lets double\n\n# Step 4: Visualize results\nlibrary(ggplot2)\n\n# Plot glucose vs. predicted probability\nggplot(PimaIndiansDiabetesWithPred, aes(x = glucose, y = diabetes_dummy)) +\n  geom_point(alpha = 0.5) +\n  geom_point(aes(x = glucose, y = predicted_probability),alpha = 0.5, colour = \"red\") +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial), se = FALSE) +\n  labs(\n       x = \"Glucose\",\n       y = \"Diabetes\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nNow build a logistic regression model with more features. Include glucose + age + mass. Compare predictions!\n\n# Fit a logistic regression model\nlogistic_model_three &lt;- glm(diabetes_dummy ~ glucose + age + mass, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model_three)\n\n\nCall:\nglm(formula = diabetes_dummy ~ glucose + age + mass, family = binomial, \n    data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.393743   0.666067 -12.602  &lt; 2e-16 ***\nglucose      0.032512   0.003329   9.767  &lt; 2e-16 ***\nage          0.030157   0.007632   3.951 7.77e-05 ***\nmass         0.081590   0.013526   6.032 1.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 755.68  on 764  degrees of freedom\nAIC: 763.68\n\nNumber of Fisher Scoring iterations: 5\n\n#Continue!\n\nNow lets do the same in python.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "2_EvaluationMetrics.html",
    "href": "2_EvaluationMetrics.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "2_EvaluationMetrics.html#include-more-features",
    "href": "2_EvaluationMetrics.html#include-more-features",
    "title": "",
    "section": "Include more features",
    "text": "Include more features\n\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n\n\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes, prop = 0.7)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nmodel_class_more &lt;- glm(diabetes ~ age + pressure + triceps + mass + insulin, family = \"binomial\", data = PimaIndiansDiabetes)\nsummary(model_class_more)\n\n\nCall:\nglm(formula = diabetes ~ age + pressure + triceps + mass + insulin, \n    family = \"binomial\", data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.128249   0.543035  -9.444  &lt; 2e-16 ***\nage          0.049228   0.007369   6.681 2.38e-11 ***\npressure    -0.008907   0.004768  -1.868  0.06176 .  \ntriceps     -0.007765   0.006060  -1.281  0.20006    \nmass         0.104439   0.013701   7.623 2.48e-14 ***\ninsulin      0.002179   0.000784   2.779  0.00545 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 863.81  on 762  degrees of freedom\nAIC: 875.81\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe have our fitted coefficients! What do they mean? We can have a guide of the association between outcome label and predictors\n\ntest_data &lt;-  test_data %&gt;% \n  mutate(pred_diab = as.numeric(predict(model_class_more, test_data, type = \"response\")), \n         pred_class =  as.factor(ifelse(pred_diab &gt; 0.5, \"pos\", \"neg\")))\n  \nhead(test_data)\n\n   pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1         6     148       72      35       0 33.6    0.627  50      pos\n3         8     183       64       0       0 23.3    0.672  32      pos\n4         1      89       66      23      94 28.1    0.167  21      neg\n9         2     197       70      45     543 30.5    0.158  53      pos\n15        5     166       72      19     175 25.8    0.587  51      pos\n17        0     118       84      47     230 45.8    0.551  31      pos\n   pred_diab pred_class\n1  0.4823141        neg\n3  0.1558332        neg\n4  0.1516937        neg\n9  0.7060717        pos\n15 0.4180845        neg\n17 0.6385543        pos\n\n\n\ntrain_data &lt;-  train_data %&gt;% \n  mutate(pred_diab = as.numeric(predict(model_class_more, train_data, type = \"response\")), \n         pred_class =  as.factor(ifelse(pred_diab &gt; 0.5, \"pos\", \"neg\")))\n  \nhead(train_data)\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n415        0     138       60      35     167 34.6    0.534  21      pos\n463        8      74       70      40      49 35.3    0.705  39      neg\n179        5     143       78       0       0 45.0    0.190  47      neg\n526        3      87       60      18       0 21.8    0.444  21      neg\n195        8      85       55      20       0 24.4    0.136  42      neg\n118        5      78       48       0       0 33.7    0.654  25      neg\n     pred_diab pred_class\n415 0.28430312        neg\n463 0.41362731        neg\n179 0.76682847        pos\n526 0.07642942        neg\n195 0.23911451        neg\n118 0.30885372        neg\n\n\n\nclass_metrics &lt;- metric_set(accuracy, precision, recall, sens, specificity)\n\n\ntest_data %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class )\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.680\n2 precision   binary         0.713\n3 recall      binary         0.847\n4 sens        binary         0.847\n5 specificity binary         0.370\n\n\n\ntrain_data %&gt;%\n  class_metrics(truth = diabetes, estimate = pred_class )\n\n# A tibble: 5 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.698\n2 precision   binary         0.721\n3 recall      binary         0.877\n4 sens        binary         0.877\n5 specificity binary         0.364\n\n\nSlight increase in performance."
  },
  {
    "objectID": "1_EvaluationMetrics.html",
    "href": "1_EvaluationMetrics.html",
    "title": "",
    "section": "",
    "text": "CodeEvaluate performance in regression\n\n# Load required libraries\nlibrary(mlbench)\nlibrary(tidymodels)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n# Load the dataset and view\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n#Lets keep only the variables we are interested in \n\nPimaIndiansDiabetes_Small &lt;- PimaIndiansDiabetes %&gt;%\n  select(age, glucose, diabetes, mass)\n\n\n# Split into training and testing datasets\nset.seed(123)\n\ndata_split &lt;- initial_split(PimaIndiansDiabetes_Small, prop = 0.7)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\nCheck dimensions of both! Want to split original dataset in a 70%, 30%\n\ndim(PimaIndiansDiabetes_Small)\n\n[1] 768   4\n\ndim(train_data)\n\n[1] 537   4\n\ndim(test_data)\n\n[1] 231   4\n\n\nNow imagine we believe that the relationship between glucose and age is cubic. We can fit a polynomial like in class:\n\nmodel_cubic &lt;- lm(glucose ~ poly(age, 3), data = train_data)\nsummary(model_cubic)\n\n\nCall:\nlm(formula = glucose ~ poly(age, 3), data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-128.659  -20.660   -3.134   18.239   85.960 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    121.663      1.336  91.044  &lt; 2e-16 ***\npoly(age, 3)1  183.577     30.967   5.928 5.51e-09 ***\npoly(age, 3)2  -35.517     30.967  -1.147    0.252    \npoly(age, 3)3   -5.328     30.967  -0.172    0.863    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.97 on 533 degrees of freedom\nMultiple R-squared:  0.06407,   Adjusted R-squared:  0.05881 \nF-statistic: 12.16 on 3 and 533 DF,  p-value: 1.039e-07\n\n\nNow, we want to understand if the model will be able to generalize to unseen data. In other words, we want to understand if we are overfitting or underfitting our data. To study this, we are going to evaluate the performance of our model in both our training (where the model is fit, and best fit parameters determined) and our testing data (not part of the fitting data at all!)\nAs this is a regression model, we can calculate our well-known metrics RMSE, MSE, SSR .. that measure the error between predicted y (glucose) and actual glucose value. We can do this by calculating the metrics ourselves, or using the many already determined functions.\nBut first, lets make our predictions:\n\ntraining_glucose_prediction &lt;-  as.numeric(predict(model_cubic, train_data))\n\nhead(training_glucose_prediction) \n\n[1] 112.1167 127.2420 132.4825 112.1167 129.3420 115.7673\n\n\nTo make it easier for us, lets include this predicted glucose in our training data as an extra column\n\ntrain_data &lt;- cbind(train_data, pred_glucose =training_glucose_prediction )\n\nhead(train_data)\n\n    age glucose diabetes mass pred_glucose\n415  21     138      pos 34.6     112.1167\n463  39      74      neg 35.3     127.2420\n179  47     143      neg 45.0     132.4825\n526  21      87      neg 21.8     112.1167\n195  42      85      neg 24.4     129.3420\n118  25      78      neg 33.7     115.7673\n\n\nNow lets do the same for the testing data\n\ntesting_glucose_prediction &lt;-  as.numeric(predict(model_cubic, test_data))\n\ntest_data &lt;- cbind(test_data, pred_glucose = testing_glucose_prediction )\n\nhead(test_data)\n\n   age glucose diabetes mass pred_glucose\n1   50     148      pos 33.6     134.1276\n3   32     183      pos 23.3     121.8059\n4   21      89      neg 28.1     112.1167\n9   53     197      pos 30.5     135.5756\n15  51     166      pos 25.8     134.6329\n17  31     118      pos 45.8     120.9758\n\n\nOnto evaluating our performance:\n\n# Function to calculate RMSE\nrmse_mine &lt;- function(actual, predicted) {\n  sqrt(mean((actual - predicted)^2))\n}\n\nTrain:\n\nrmse_mine(train_data$glucose, train_data$pred_glucose )\n\n[1] 30.85099\n\n\nTest:\n\nrmse_mine(test_data$glucose, test_data$pred_glucose )\n\n[1] 30.60295\n\n\nCan use already predefined functions (e.g Metrics package https://cran.r-project.org/web/packages/Metrics/Metrics.pdf)\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    accuracy, mae, mape, mase, precision, recall, rmse, smape\n\n\n\n# Calculate individual metrics: train\nMetrics::rmse(train_data$glucose, train_data$pred_glucose ) # the dots mean that I want this function rmse, to come from this package! In case other functions are named the same way\n\n[1] 30.85099\n\nMetrics::mae(train_data$glucose, train_data$pred_glucose )\n\n[1] 23.68902\n\n\n\n# Calculate individual metrics: test\nMetrics::rmse(test_data$glucose, test_data$pred_glucose)\n\n[1] 30.60295\n\nMetrics::mae(test_data$glucose, test_data$pred_glucose )\n\n[1] 24.59446\n\n\nDo not worry about trying to replicate the plots!\n\n# Visualise models\nggplot(train_data, aes(x = age, y = glucose)) +\n  geom_point(color = \"red\", alpha = 0.6) +\n  geom_line(data = data.frame(\n    age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100),\n    glucose = predict(model_cubic, newdata = data.frame(age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100)))\n  ), aes(x = age, y = glucose), color = \"blue\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Degree 3 Fit\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n# Visualise models\nggplot(test_data, aes(x = age, y = glucose)) +\n  geom_point(color = \"grey\", alpha = 0.6) +\n  geom_line(data = data.frame(\n    age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100),\n    glucose = predict(model_cubic, newdata = data.frame(age = seq(min(PimaIndiansDiabetes_Small$age), max(PimaIndiansDiabetes_Small$age), length.out = 100)))\n  ), aes(x = age, y = glucose), color = \"blue\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Degree 3 Fit\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThey both fit the data in quite a similar way ! Overfitting, not happening as similar performance in both train and test, but probably this is not the best model, underfitting as missing lots of insight and error is high. What can be the reason behind this? My go to, is that we should increase number of features, as trying to predict glucose with age is too simple and cannot capture the complexity of glucose behaviour.\n\nKey things to understand: - Dataset splitting - Fitting model in training - Evaluating performance of fitted model in both train and test to understand possible overfitting, underfitting issues - Extract insight from modeling and decide next steps\n\n\n\n\n Back to top"
  },
  {
    "objectID": "3_Confusion.html",
    "href": "3_Confusion.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "3_Confusion.html#confusion-matrices",
    "href": "3_Confusion.html#confusion-matrices",
    "title": "",
    "section": "Confusion Matrices",
    "text": "Confusion Matrices\nOn Friday you saw a very rudimentary confusion matrix done with the function table() as we learnt with correlation matrices last Tuesday, we can integrate already developed functions and packages to enhance our analysis\nHere is a set of different confusion matrices and an exercise at the end to experiment with what happens when thresholds change.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(tidymodels)\nlibrary(ConfusionTableR) #probably will need to install it\n\nLets create a sample dataset - similar to the diabetes dataset with made up probabilities:\n\n# Example data with predicted probabilities\nset.seed(123)\nactual &lt;- factor(sample(c(\"pos\", \"neg\"), 20, replace = TRUE, prob = c(0.5, 0.5)))\npredicted_prob &lt;- runif(20) # Simulated predicted probabilities\n\nChoose your threshold (e.g 0.5 to start)\n\n# Adjust threshold and classify\nthreshold &lt;- 0.5 \npredicted &lt;- factor(ifelse(predicted_prob &gt;= threshold, \"pos\", \"neg\"))\n\nConfusion matrix using tidymodels\n\ndata &lt;- data.frame(actual = actual, predicted = predicted)\n\ncm &lt;- data %&gt;%\n  conf_mat(truth = actual, estimate = predicted)\n\ncm\n\n          Truth\nPrediction neg pos\n       neg   4   3\n       pos   5   8\n\n\n\nautoplot(cm, type = \"heatmap\")\n\n\n\n\n\n\n\n\nConfusionTableR::binary_visualiseR(train_labels = predicted,\n                                   truth_labels= actual,\n                                   class_label1 = \"Negative\", \n                                   class_label2 = \"Positive\",\n                                   quadrant_col1 = \"#28ACB4\", \n                                   quadrant_col2 = \"#4397D2\", \n                                   custom_title = \"Diabetes Confusion Matrix\", \n                                   text_col= \"black\")\n\n\n\n\n\n\n\nChange your threshold to 0.3, what changes?\nNow same thing with 0.8"
  },
  {
    "objectID": "Basic_Understanding_Python.html",
    "href": "Basic_Understanding_Python.html",
    "title": "(1b) Fitting a logistic regression model - Python",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\n\n\n# Load the Pima Indians Diabetes dataset\ndata = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\nPimaIndiansDiabetes = data.frame\n\nWe have some different names (plas is glucose in R), but same idea.\n\n# Select relevant features and target variable\nPimaIndiansDiabetes = PimaIndiansDiabetes[[\"plas\", \"class\", \"mass\", \"age\"]]\n\n\n# Define predictor (X) and target (y) variables\nX = PimaIndiansDiabetes[[\"plas\"]]\ny = PimaIndiansDiabetes[\"class\"]\n\n\n# Fit a logistic regression model using sklearn\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n\n# Display model coefficients\nintercept = logistic_model.intercept_[0]\nslope = logistic_model.coef_[0][0]\nprint(f\"Intercept: {intercept}, Slope: {slope}\")\n\nIntercept: -5.349905858014434, Slope: 0.037871691000185226\n\n\n\n# Predict probabilities\nPimaIndiansDiabetes[\"predicted_probability\"] = logistic_model.predict_proba(X)[:, 1]\n\n&lt;string&gt;:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nPimaIndiansDiabetes.head()\n\n   plas            class  mass  age  predicted_probability\n0   148  tested_positive  33.6   50               0.563432\n1    85  tested_negative  26.6   31               0.106138\n2   183  tested_positive  23.3   32               0.829291\n3    89  tested_negative  28.1   21               0.121391\n4   137  tested_positive  43.1   33               0.459716\n\n\n\n# Visualise predictions\nplt.figure(figsize=(8, 6))\nplt.scatter(PimaIndiansDiabetes[\"plas\"], PimaIndiansDiabetes[\"class\"], alpha=0.5, label=\"Actual\")\nplt.scatter(PimaIndiansDiabetes[\"plas\"], PimaIndiansDiabetes[\"predicted_probability\"], color=\"red\", alpha=0.5, label=\"Predicted\")\nplt.plot(\n    np.sort(PimaIndiansDiabetes[\"plas\"]),\n    np.sort(logistic_model.predict_proba(X)[:, 1]),\n    color=\"blue\",\n    label=\"Logistic Regression Curve\"\n)\nplt.xlabel(\"Glucose\")\nplt.ylabel(\"Diabetes (0 or 1)\")\nplt.title(\"Logistic Regression: Glucose vs Diabetes\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nNow include more predictors: “plas” and “age”\n\n# Fit logistic regression with multiple predictors\nX_multi = PimaIndiansDiabetes[[\"plas\", \"age\"]]\nlogistic_model_two = LogisticRegression()\nlogistic_model_two.fit(X_multi, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n\n# Display model coefficients\nintercept = logistic_model_two.intercept_[0]\nslope = logistic_model_two.coef_[0][0]\nprint(f\"Intercept: {intercept}, Slope: {slope}\")\n\nIntercept: -5.912369223097624, Slope: 0.035643719099126295\n\n\n\n# Predict probabilities for the multi-feature model\nPimaIndiansDiabetes[\"predicted_probability_two\"] = logistic_model_two.predict_proba(X_multi)[:, 1]\n\n&lt;string&gt;:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nVisualize the probabilities\n\nPimaIndiansDiabetes.head()\n\n   plas            class  ...  predicted_probability  predicted_probability_two\n0   148  tested_positive  ...               0.563432                   0.646059\n1    85  tested_negative  ...               0.106138                   0.107690\n2   183  tested_positive  ...               0.829291                   0.802707\n3    89  tested_negative  ...               0.121391                   0.097990\n4   137  tested_positive  ...               0.459716                   0.447313\n\n[5 rows x 6 columns]\n\n\n\nAdd mass to the modeling too (so in total three predictors, one outcome variable), how do the predictions change?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "GradientAscent_R_Laura.html",
    "href": "GradientAscent_R_Laura.html",
    "title": "",
    "section": "",
    "text": "Code\n# Load library\nlibrary(rgl)\n\n# Define sigmoid function\nsigmoid &lt;- function(z) {\n  1 / (1 + exp(-z))\n}\n\n# Define log-likelihood function\nlog_likelihood &lt;- function(X, y, theta) {\n  z &lt;- X %*% theta\n  sum(y * log(sigmoid(z)) + (1 - y) * log(1 - sigmoid(z)))\n}\n\n# Gradient ascent function with tracking\ngradient_ascent_with_tracking &lt;- function(X, y, alpha = 0.1, iterations = 50) {\n  theta &lt;- matrix(0, ncol = 1, nrow = ncol(X))  # Initialize parameters\n  tracked_thetas &lt;- list(theta)  # Store theta values\n  \n  for (i in 1:iterations) {\n    z &lt;- X %*% theta\n    gradient &lt;- t(X) %*% (y - sigmoid(z))  # Compute gradient\n    theta &lt;- theta + alpha * gradient      # Update parameters\n    tracked_thetas[[i + 1]] &lt;- theta       # Store theta\n  }\n  \n  list(theta = theta, tracked_thetas = tracked_thetas)\n}\n\n\n# Dataset\nX &lt;- matrix(c(0.5, 1.5,\n              1.0, 1.0,\n              1.5, 0.5,\n              2.0, 2.0,\n              2.5, 2.5,\n              3.0, 0.5), ncol = 2, byrow = TRUE)\n\ny &lt;- c(0, 0, 0, 1, 1, 1)  # Labels\n\n# Add intercept column to X\nX_with_intercept &lt;- cbind(1, X)\n\n\n# Perform gradient ascent and track points\nresult &lt;- gradient_ascent_with_tracking(X_with_intercept, y)\ntheta_opt &lt;- result$theta\ntracked_thetas &lt;- result$tracked_thetas\n\n# Extract tracked theta values for plotting\ntracked_theta_1 &lt;- sapply(tracked_thetas, function(t) t[2])\ntracked_theta_2 &lt;- sapply(tracked_thetas, function(t) t[3])\ntracked_log_likelihood &lt;- sapply(tracked_thetas, function(t) log_likelihood(X_with_intercept, y, t))\n\n# Generate a grid of theta values for plotting\ntheta_1_vals &lt;- seq(-2, 2, length.out = 50)\ntheta_2_vals &lt;- seq(-2, 2, length.out = 50)\nlog_likelihood_vals &lt;- matrix(0, nrow = length(theta_1_vals), ncol = length(theta_2_vals))\n\nfor (i in 1:length(theta_1_vals)) {\n  for (j in 1:length(theta_2_vals)) {\n    theta &lt;- matrix(c(0, theta_1_vals[i], theta_2_vals[j]), ncol = 1)\n    log_likelihood_vals[i, j] &lt;- log_likelihood(X_with_intercept, y, theta)\n  }\n}\n\n\npersp3d(theta_1_vals, theta_2_vals, t(log_likelihood_vals), \n        col = \"lightblue\", alpha = 0.6, xlab = \"Theta 1\", ylab = \"Theta 2\", zlab = \"Log-Likelihood\")\n\n# Add gradient ascent path to persp3d\npoints3d(tracked_theta_1, tracked_theta_2, tracked_log_likelihood, col = \"red\", size = 10)\n\n\n### # Plot the 3D surface with gradient ascent points\n### rgl.open()\n### rgl.surface(theta_1_vals, theta_2_vals, t(log_likelihood_vals), color = \"blue\", alpha = 0.6)\n### \n### # Plot gradient ascent path\n### rgl.points(tracked_theta_1, tracked_theta_2, tracked_log_likelihood, color = \"red\", size = 8)\n### \n### # Add labels\n### rgl.texts3d(tracked_theta_1, tracked_theta_2, tracked_log_likelihood,\n###             text = 1:length(tracked_log_likelihood), color = \"black\")\n### rgl.title(\"Log-Likelihood Function with Gradient Ascent Path\")\n### \n### # Add axes labels\n### rgl.viewpoint(theta = 30, phi = 30)\n### rgl.axes3d()\n### rgl.bringtotop()\n\nlibrary(rgl)\n\n# Open a new 3D device\nopen3d()\n\nglX \n  2 \n\n# 3D surface\nsurface3d(\n  x = theta_1_vals,\n  y = theta_2_vals,\n  z = t(log_likelihood_vals),\n  color = \"blue\",\n  alpha = 0.6\n)\n\n# Gradient ascent path as points\npoints3d(\n  tracked_theta_1,\n  tracked_theta_2,\n  tracked_log_likelihood,\n  color = \"red\",\n  size = 8\n)\n\n# Labels for each point (1, 2, 3, ...)\ntext3d(\n  tracked_theta_1,\n  tracked_theta_2,\n  tracked_log_likelihood,\n  texts = 1:length(tracked_log_likelihood),\n  color = \"black\"\n)\n\n# Title + axis labels\ntitle3d(\n  main = \"Log-Likelihood Function with Gradient Ascent Path\",\n  xlab = expression(theta[1]),\n  ylab = expression(theta[2]),\n  zlab = \"Log-likelihood\"\n)\n\n# Axes and viewpoint\naxes3d()\naspect3d(1, 1, 1)      # optional: equal aspect ratio\nrgl.viewpoint(theta = 30, phi = 30)\n\nWarning in rgl.viewpoint(theta = 30, phi = 30): 'rgl.viewpoint' is deprecated.\nUse 'view3d' instead.\nSee help(\"Deprecated\")\n\nrgl.bringtotop()\n\n\n# Load library\nlibrary(ggplot2)\n\n# Define sigmoid function\nsigmoid &lt;- function(z) {\n  1 / (1 + exp(-z))\n}\n\n# Define decision boundary function\ndecision_boundary &lt;- function(beta, x_vals) {\n  -(beta[1] + beta[2] * x_vals) / beta[3]\n}\n\n# Dataset\nX &lt;- data.frame(X1 = c(0.5, 1.0, 1.5, 2.0, 2.5, 3.0),\n                X2 = c(1.5, 1.0, 0.5, 2.0, 2.5, 0.5),\n                y = factor(c(0, 0, 0, 1, 1, 1)))  # Labels as factor\n\n# Beta combinations\nbeta1 &lt;- c(-3, 1, 1)  # Intercept = -3, Coefficients = [1, 1]\nbeta2 &lt;- c(-4, 2, 1)  # Intercept = -4, Coefficients = [2, 1]\n\n# Generate x values for plotting the decision boundaries\nx_vals &lt;- seq(0, 4, length.out = 100)\n\n# Compute y values for both decision boundaries\ndecision1 &lt;- data.frame(X1 = x_vals, X2 = decision_boundary(beta1, x_vals), Boundary = \"b = -3\")\ndecision2 &lt;- data.frame(X1 = x_vals, X2 = decision_boundary(beta2, x_vals), Boundary = \"b = -4\")\n\n# Combine decision boundaries into one data frame\ndecision_boundaries &lt;- rbind(decision1, decision2)\n\n# Plot the data points and decision boundaries\nggplot(data = X, aes(x = X1, y = X2, color = y)) +\n  geom_point(size = 4, shape = 19) +\n  geom_line(data = decision_boundaries, aes(x = X1, y = X2, linetype = Boundary), color = \"blue\", size = 1) +\n  scale_color_manual(values = c(\"blue\", \"red\"), labels = c(\"y = 0\", \"y = 1\")) +\n  labs(title = \"Decision Boundaries for Different Betas\",\n       x = \"X1\", y = \"X2\", color = \"Labels\", linetype = \"Beta Combinations\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, size = 16),\n        legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "LogisticRegression.html",
    "href": "LogisticRegression.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "LogisticRegression.html#classes-powerpoint-presentation",
    "href": "LogisticRegression.html#classes-powerpoint-presentation",
    "title": "",
    "section": "Classes PowerPoint Presentation",
    "text": "Classes PowerPoint Presentation\n\n\n#| echo: false #| results: hide #| output: false #| include: false\n\nlibrary(tidyverse) \nlibrary(caret) \ntheme_set(theme_bw())\n\nLoad the data and remove NAs\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\nPimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)\n\nInspect the data\n\nsample_n(PimaIndiansDiabetes2, 3)\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n689        1     140       74      26     180 24.1    0.828  23      neg\n261        3     191       68      15     130 30.9    0.299  34      neg\n53         5      88       66      21      23 24.4    0.342  30      neg\n\n\nSplit the data into training and test set\n\nset.seed(123)\ntraining.samples &lt;- PimaIndiansDiabetes2$diabetes %&gt;% \n  createDataPartition(p = 0.8, list = FALSE)\ntrain.data  &lt;- PimaIndiansDiabetes2[training.samples, ]\ntest.data &lt;- PimaIndiansDiabetes2[-training.samples, ]\n\nFit the model\n\nmodel &lt;- glm( diabetes ~., data = train.data, family = binomial)\n\nSummarize the model\n\nsummary(model)\n\n\nCall:\nglm(formula = diabetes ~ ., family = binomial, data = train.data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.053e+01  1.440e+00  -7.317 2.54e-13 ***\npregnant     1.005e-01  6.127e-02   1.640  0.10092    \nglucose      3.710e-02  6.486e-03   5.719 1.07e-08 ***\npressure    -3.876e-04  1.383e-02  -0.028  0.97764    \ntriceps      1.418e-02  1.998e-02   0.710  0.47800    \ninsulin      5.940e-04  1.508e-03   0.394  0.69371    \nmass         7.997e-02  3.180e-02   2.515  0.01190 *  \npedigree     1.329e+00  4.823e-01   2.756  0.00585 ** \nage          2.718e-02  2.020e-02   1.346  0.17840    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 398.80  on 313  degrees of freedom\nResidual deviance: 267.18  on 305  degrees of freedom\nAIC: 285.18\n\nNumber of Fisher Scoring iterations: 5\n\n\nMake predictions\n\nprobabilities &lt;- model %&gt;% predict(test.data, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\n\nModel accuracy\n\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7564103\n\nmodel &lt;- glm( diabetes ~ glucose, data = train.data, family = binomial)\nsummary(model)$coef\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -6.15882009 0.700096646 -8.797100 1.403974e-18\nglucose      0.04327234 0.005341133  8.101716 5.418949e-16\n\n\n\nnewdata &lt;- data.frame(glucose = c(20,  180))\nprobabilities &lt;- model %&gt;% predict(newdata, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\npredicted.classes\n\n    1     2 \n\"neg\" \"pos\" \n\ntrain.data %&gt;%\n  mutate(prob = ifelse(diabetes == \"pos\", 1, 0)) %&gt;%\n  ggplot(aes(glucose, prob)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(\n    title = \"Logistic Regression Model\", \n    x = \"Plasma Glucose Concentration\",\n    y = \"Probability of being diabete-pos\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nmodel &lt;- glm( diabetes ~ glucose + mass + pregnant, \n              data = train.data, family = binomial)\nsummary(model)$coef\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -9.32369818 1.125997285 -8.280391 1.227711e-16\nglucose      0.03886154 0.005404219  7.190962 6.433636e-13\nmass         0.09458458 0.023529905  4.019760 5.825738e-05\npregnant     0.14466661 0.045125729  3.205857 1.346611e-03\n\nmodel &lt;- glm( diabetes ~., data = train.data, family = binomial)\nsummary(model)$coef\n\n                 Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept) -1.053400e+01 1.439679266 -7.31690975 2.537464e-13\npregnant     1.005031e-01 0.061266974  1.64041157 1.009196e-01\nglucose      3.709621e-02 0.006486093  5.71934633 1.069346e-08\npressure    -3.875933e-04 0.013826185 -0.02803328 9.776356e-01\ntriceps      1.417771e-02 0.019981885  0.70952823 4.779967e-01\ninsulin      5.939876e-04 0.001508231  0.39383055 6.937061e-01\nmass         7.997447e-02 0.031798907  2.51500698 1.190300e-02\npedigree     1.329149e+00 0.482291020  2.75590704 5.852963e-03\nage          2.718224e-02 0.020199295  1.34570257 1.783985e-01\n\ncoef(model)\n\n  (Intercept)      pregnant       glucose      pressure       triceps \n-1.053400e+01  1.005031e-01  3.709621e-02 -3.875933e-04  1.417771e-02 \n      insulin          mass      pedigree           age \n 5.939876e-04  7.997447e-02  1.329149e+00  2.718224e-02 \n\nmodel &lt;- glm( diabetes ~ pregnant + glucose + pressure + mass + pedigree, \n              data = train.data, family = binomial)\n\nprobabilities &lt;- model %&gt;% predict(test.data, type = \"response\")\nhead(probabilities)\n\n       19        21        32        55        64        71 \n0.1352603 0.5127526 0.6795376 0.7517408 0.2734867 0.1648174 \n\ncontrasts(test.data$diabetes)\n\n    pos\nneg   0\npos   1\n\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\nhead(predicted.classes)\n\n   19    21    32    55    64    71 \n\"neg\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \n\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7564103\n\nlibrary(\"mgcv\")\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n\n# Fit the model\ngam.model &lt;- gam(diabetes ~ s(glucose) + mass + pregnant,\n                 data = train.data, family = \"binomial\")\n# Summarize model\nsummary(gam.model )\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\ndiabetes ~ s(glucose) + mass + pregnant\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.59794    0.86982  -5.286 1.25e-07 ***\nmass         0.09458    0.02353   4.020 5.83e-05 ***\npregnant     0.14467    0.04513   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df Chi.sq p-value    \ns(glucose)   1      1  51.71  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.339   Deviance explained = 29.8%\nUBRE = -0.083171  Scale est. = 1         n = 314\n\n# Make predictions\nprobabilities &lt;- gam.model %&gt;% predict(test.data, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities&gt; 0.5, \"pos\", \"neg\")\n# Model Accuracy\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7820513"
  },
  {
    "objectID": "LogisticRegression.html#optional",
    "href": "LogisticRegression.html#optional",
    "title": "",
    "section": "optional ========================================================================",
    "text": "optional ========================================================================\nExample implementation from scratch:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nN &lt;- 200 # number of points per class\nD &lt;- 2 # dimensionality, we use 2D data for easy visualization\nK &lt;- 2 # number of classes, binary for logistic regression\nX &lt;- data.frame() # data matrix (each row = single example, can view as xy coordinates)\ny &lt;- data.frame() # class labels\n\nset.seed(56)\n\nt &lt;- seq(0,1,length.out = N) \nfor (j in (1:K)){\n  # t, m are parameters of parametric equations x1, x2\n  # t &lt;- seq(0,1,length.out = N) \n  # add randomness \n  m &lt;- rnorm(N, j+0.5, 0.25) \n  Xtemp &lt;- data.frame(x1 = 3*t , x2 = m - t) \n  ytemp &lt;- data.frame(matrix(j-1, N, 1))\n  X &lt;- rbind(X, Xtemp)\n  y &lt;- rbind(y, ytemp)\n}\n\ndata &lt;- cbind(X,y)\ncolnames(data) &lt;- c(colnames(X), 'label')\n\n# create dir images\n#dir.create(file.path('.', 'images'), showWarnings = FALSE)\n\n# lets visualize the data:\ndata_plot &lt;- ggplot(data) + geom_point(aes(x=x1, y=x2, color = as.character(label)), size = 2) + \n  scale_colour_discrete(name  =\"Label\") + \n  ylim(0, 3) + coord_fixed(ratio = 1) +\n  ggtitle('Data to be classified') +\n  theme_bw(base_size = 12) +\n  theme(legend.position=c(0.85, 0.87))\n\n#png(file.path('images', 'data_plot.png'))\nprint(data_plot)\n\n\n\n\n\n\n#dev.off()\n\n#sigmoid function, inverse of logit\nsigmoid &lt;- function(z){1/(1+exp(-z))}\n\n#cost function\ncost &lt;- function(theta, X, y){\n  m &lt;- length(y) # number of training examples\n  h &lt;- sigmoid(X %*% theta)\n  J &lt;- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m\n  J\n}\n\n#gradient function\ngrad &lt;- function(theta, X, y){\n  m &lt;- length(y) \n  \n  h &lt;- sigmoid(X%*%theta)\n  grad &lt;- (t(X)%*%(h - y))/m\n  grad\n}\n\nlogisticReg &lt;- function(X, y){\n  #remove NA rows\n  X &lt;- na.omit(X)\n  y &lt;- na.omit(y)\n  #add bias term and convert to matrix\n  X &lt;- mutate(X, bias =1)\n  #move the bias column to col1\n  X &lt;- as.matrix(X[, c(ncol(X), 1:(ncol(X)-1))])\n  y &lt;- as.matrix(y)\n  #initialize theta\n  theta &lt;- matrix(rep(0, ncol(X)), nrow = ncol(X))\n  #use the optim function to perform gradient descent\n  costOpti &lt;- optim(theta, fn = cost, gr = grad, X=X, y=y)\n  #return coefficients\n  return(costOpti$par)\n}\n\nlogisticProb &lt;- function(theta, X){\n  X &lt;- na.omit(X)\n  #add bias term and convert to matrix\n  X &lt;- mutate(X, bias =1)\n  X &lt;- as.matrix(X[,c(ncol(X), 1:(ncol(X)-1))])\n  return(sigmoid(X%*%theta))\n}\n\nlogisticPred &lt;- function(prob){\n  return(round(prob, 0))\n}\n\n# training\ntheta &lt;- logisticReg(X, y)\nprob &lt;- logisticProb(theta, X)\npred &lt;- logisticPred(prob)\n\n# generate a grid for decision boundary, this is the test set\ngrid &lt;- expand.grid(seq(0, 3, length.out = 100), seq(0, 3, length.out = 100))\n# predict the probability\nprobZ &lt;- logisticProb(theta, grid)\n# predict the label\nZ &lt;- logisticPred(probZ)\ngridPred = cbind(grid, Z)\n\n# decision boundary visualization\np &lt;- ggplot() + geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) + \n  geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F)+ \n  ylim(0, 3) +\n  ggtitle('Decision Boundary for Logistic Regression') +\n  coord_fixed(ratio = 1) +\n  theme_bw(base_size = 12) \n\n#png(file.path('images', 'logistic_regression.png'))\nprint(p)\n\nWarning: Removed 200 rows containing missing values or values outside the scale range\n(`geom_tile()`).\n\n\n\n\n\n\n\n#dev.off()"
  },
  {
    "objectID": "logistic_regression_tidymodels.html",
    "href": "logistic_regression_tidymodels.html",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "",
    "text": "Before fitting any model, we need to be clear about how probabilities, odds and log-odds (logits) are related.\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.2f\\n\", prob))\n\nProbability: 0.80\n\n# Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.2f\\n\", odds))\n\nOdds: 4.00\n\n# Convert odds to log-odds (logit)\nlog_odds &lt;- log(odds)\ncat(sprintf(\"Log-odds: %.2f\\n\", log_odds))\n\nLog-odds: 1.39\n\n\n\n\nProbability: chances of the event happening, between 0 and 1.\n\n\nOdds: ratio of probability of the event to probability of no event\n[ = ]\n\nLog-odds (logit): natural log of the odds\n[ (p) = () ]\n\nLog-odds are unbounded (from (-) to (+)) and this is what the logistic regression model is linear in.",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#probabilities-odds-and-log-odds",
    "href": "logistic_regression_tidymodels.html#probabilities-odds-and-log-odds",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "",
    "text": "Before fitting any model, we need to be clear about how probabilities, odds and log-odds (logits) are related.\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.2f\\n\", prob))\n\nProbability: 0.80\n\n# Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.2f\\n\", odds))\n\nOdds: 4.00\n\n# Convert odds to log-odds (logit)\nlog_odds &lt;- log(odds)\ncat(sprintf(\"Log-odds: %.2f\\n\", log_odds))\n\nLog-odds: 1.39\n\n\n\n\nProbability: chances of the event happening, between 0 and 1.\n\n\nOdds: ratio of probability of the event to probability of no event\n[ = ]\n\nLog-odds (logit): natural log of the odds\n[ (p) = () ]\n\nLog-odds are unbounded (from (-) to (+)) and this is what the logistic regression model is linear in.",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#data-pima-indians-diabetes",
    "href": "logistic_regression_tidymodels.html#data-pima-indians-diabetes",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "2. Data: Pima Indians Diabetes",
    "text": "2. Data: Pima Indians Diabetes\nWe will use the PimaIndiansDiabetes data from the mlbench package and keep only a few variables for now as in other exercises.\n\nlibrary(mlbench)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes, mass, age)\n\nglimpse(pima)\n\nRows: 768\nColumns: 4\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n\n\n\n\nOutcome: diabetes (factor with levels neg and pos)\n\nPredictors: glucose, mass (BMI), age\n\n\nLet’s quickly check class balance:\n\npima %&gt;%\n  count(diabetes)\n\n  diabetes   n\n1      neg 500\n2      pos 268\n\n\nBy default, tidymodels will treat the first level of the factor as the “event of interest”.\nHere, \"neg\" comes first, but we want \"pos\" (diabetes present) to be the event.\n\npima &lt;- pima %&gt;%\n  mutate(diabetes = fct_relevel(diabetes, \"pos\"))\n\npima |&gt;\n  count(diabetes)\n\n  diabetes   n\n1      pos 268\n2      neg 500\n\n\nNow \"pos\" is the first level and will be the class for which probabilities are predicted.",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#logistic-regression-with-tidymodels-one-predictor",
    "href": "logistic_regression_tidymodels.html#logistic-regression-with-tidymodels-one-predictor",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "3. Logistic regression with tidymodels (one predictor)",
    "text": "3. Logistic regression with tidymodels (one predictor)\nWe start with a simple model: predict diabetes from glucose only.\n3.1 Specify the model\nWe use parsnip::logistic_reg() and set the engine to \"glm\" (which uses the classical GLM under the hood). Here is the link to\n\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")\n\nlogistic_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n3.2 Fit the model\nWe can fit the model directly with a formula, without recipes or workflows for now:\n\nlogistic_fit_glucose &lt;-\n  logistic_spec %&gt;%\n  fit(diabetes ~ glucose, data = pima)\n\nlogistic_fit_glucose\n\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetes ~ glucose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      glucose  \n    5.35008     -0.03787  \n\nDegrees of Freedom: 767 Total (i.e. Null);  766 Residual\nNull Deviance:      993.5 \nResidual Deviance: 808.7    AIC: 812.7\n\n\nThe fitted model stores the estimated intercept and slope for the log-odds of diabetes given glucose.\nWe can see the underlying GLM coefficients:\n\ntidy(logistic_fit_glucose)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   5.35     0.421        12.7 4.99e-37\n2 glucose      -0.0379   0.00325     -11.6 2.38e-31\n\n\nThis corresponds to the model\n[ (P( = )) = _0 + _1 . ]\nTo get the probability for a given glucose value, we apply the logistic (sigmoid) function:\n[ p( = ) = . ]",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#from-model-to-odds-and-probabilities",
    "href": "logistic_regression_tidymodels.html#from-model-to-odds-and-probabilities",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "4. From model to odds and probabilities",
    "text": "4. From model to odds and probabilities\nLet’s compute the odds of having diabetes when glucose = 120 using the fitted model.\n\ncoefs &lt;- tidy(logistic_fit_glucose)\n\n#This is the same as in yesterday's practical doing logistic_fit_glucose[[\"fit]][[\"coefficients]]\nintercept &lt;- coefs$estimate[coefs$term == \"(Intercept)\"]\nslope     &lt;- coefs$estimate[coefs$term == \"glucose\"]\n\nglucose_value &lt;- 120\n\nlog_odds_120 &lt;- intercept + slope * glucose_value    # linear predictor z\nodds_120     &lt;- exp(log_odds_120)                    # odds\nprob_120     &lt;- 1 / (1 + exp(-log_odds_120))         # probability\n\ncat(\"Log-odds at glucose = 120:\", round(log_odds_120, 3), \"\\n\")\n\nLog-odds at glucose = 120: 0.805 \n\ncat(\"Odds at glucose = 120    :\", round(odds_120, 3), \"\\n\")\n\nOdds at glucose = 120    : 2.237 \n\ncat(\"Probability at glucose = 120:\", round(prob_120, 3), \"\\n\")\n\nProbability at glucose = 120: 0.691",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#making-predictions-with-tidymodels",
    "href": "logistic_regression_tidymodels.html#making-predictions-with-tidymodels",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "5. Making predictions with tidymodels",
    "text": "5. Making predictions with tidymodels\naugment() from broom/broom.mixed (loaded through tidymodels) makes it easy to attach predictions to the data.\n\npima_preds_glucose &lt;-\n  predict(logistic_fit_glucose, new_data = pima, type = \"prob\") %&gt;% # type = \"prob\" gives a column for each class probability\n  rename(prob_pos = .pred_pos, prob_neg = .pred_neg) %&gt;%\n  dplyr::bind_cols(pima)\n\npima_preds_glucose %&gt;%\n  select(glucose, diabetes, prob_pos, prob_neg) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  glucose diabetes prob_pos prob_neg\n    &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     148 pos         0.563    0.437\n2      85 neg         0.106    0.894\n3     183 pos         0.829    0.171\n4      89 neg         0.121    0.879\n5     137 pos         0.460    0.540\n6     116 neg         0.278    0.722\n\n\n5.1 Visualising the fitted curve\nWe can now visualise how the predicted probability of diabetes changes with glucose.\n\nggplot(pima_preds_glucose, aes(x = glucose, y = prob_pos)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  labs(\n    x = \"Glucose\",\n    y = \"Predicted probability of diabetes (pos)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFor comparison, overlay the observed classes (pos / neg) as 0/1:\n\npima_preds_glucose %&gt;%\n  mutate(diabetes_numeric = if_else(diabetes == \"pos\", 1, 0)) |&gt;\n  ggplot(aes(x = glucose)) +\n  geom_point(aes(y = diabetes_numeric), alpha = 0.4) +\n  geom_line(aes(y = prob_pos), colour = \"red\", linewidth = 1) +\n  scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(-0.05, 1.05)) +\n  labs(\n    x = \"Glucose\",\n    y = \"Observed class (0/1) and predicted probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nYou should see most pos cases (1) having higher predicted probabilities than neg cases (0).",
    "crumbs": [
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  }
]