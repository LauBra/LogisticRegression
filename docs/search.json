[
  {
    "objectID": "logistic_regression_python.html",
    "href": "logistic_regression_python.html",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#data-pima-indians-diabetes",
    "href": "logistic_regression_python.html#data-pima-indians-diabetes",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Data: Pima Indians Diabetes",
    "text": "Data: Pima Indians Diabetes\n\nimport pandas as pd\n\n\nurl=\"https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv\"\npima= pd.read_csv(url)\n\npima = pima[[\"Glucose\", \"Outcome\", \"BloodPressure\", \"Age\"]]\npima[\"Diabetes_binary\"] = (pima[\"Outcome\"] == 1).astype(int)\npima.head()\n\n   Glucose  Outcome  BloodPressure  Age  Diabetes_binary\n0      148        1             72   50                1\n1       85        0             66   31                0\n2      183        1             64   32                1\n3       89        0             66   21                0\n4      137        1             40   33                1",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#logistic-regression-with-scikit-learn",
    "href": "logistic_regression_python.html#logistic-regression-with-scikit-learn",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Logistic regression with scikit-learn",
    "text": "Logistic regression with scikit-learn\n\nfrom sklearn.linear_model import LogisticRegression\n\nX = pima[[\"Glucose\"]].values\ny = pima[\"Diabetes_binary\"].values\n\nlog_reg = LogisticRegression(solver=\"lbfgs\")\nlog_reg.fit(X, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\nlog_reg.intercept_, log_reg.coef_\n\n(array([-5.34990586]), array([[0.03787169]]))",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "logistic_regression_python.html#predictions-plots",
    "href": "logistic_regression_python.html#predictions-plots",
    "title": "(1a) Fitting a logistic regression model - Python (scikit-learn)",
    "section": "Predictions & plots",
    "text": "Predictions & plots\n\nprobs = log_reg.predict_proba(X)\npima[\"prob_diabetes_1\"] = probs[:, 1]\npima[\"prob_diabetes_0\"] = probs[:, 0]\npima.head()\n\n   Glucose  Outcome  ...  prob_diabetes_1  prob_diabetes_0\n0      148        1  ...         0.563432         0.436568\n1       85        0  ...         0.106138         0.893862\n2      183        1  ...         0.829291         0.170709\n3       89        0  ...         0.121391         0.878609\n4      137        1  ...         0.459716         0.540284\n\n[5 rows x 7 columns]\n\n\nWhich probability column do we want?",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - Python (scikit-learn)"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html",
    "href": "tidymodels_sklearn_overview.html",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "",
    "text": "library(reticulate)\n\n# one-off setup (if you haven't done it yet)\n# install_miniconda()\n\n##conda_create(\n##  envname = \"hds-python\",\n##  python_version = \"3.11\",\n##  packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"scikit-learn\")\n##)\n\nuse_condaenv(\"hds-python\", required = TRUE)\n#py_config()\n\n#conda_install(\"hds-python\", c(\"jupyter\", \"plotly\"))",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#why-frameworks-like-tidymodels-and-scikit-learn",
    "href": "tidymodels_sklearn_overview.html#why-frameworks-like-tidymodels-and-scikit-learn",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "1. Why frameworks like tidymodels and scikit-learn?",
    "text": "1. Why frameworks like tidymodels and scikit-learn?\nIn both R and Python, there are many ways to fit models:\n\nR: lm(), glm(), rpart(), randomForest(), xgboost(), …\nPython: statsmodels, custom NumPy/SciPy code, or many different classes in sklearn\n\n\nEach function or class often has its own syntax, argument names and prediction methods.\nThis becomes difficult when we want to:\n\nswap one model for another,\ncompare models fairly,\nbuild end-to-end pipelines with preprocessing + model + evaluation,\ndo hyperparameter tuning cleanly and without data leakage.\n\nTwo ecosystems help us solve this:\n\n\ntidymodels in R\n\n\nscikit-learn in Python\n\nBoth provide a unified, consistent interface for models, preprocessing, pipelines and tuning.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#tidymodels-r-model-specification-engine",
    "href": "tidymodels_sklearn_overview.html#tidymodels-r-model-specification-engine",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "2. Tidymodels (R): model specification + engine\n",
    "text": "2. Tidymodels (R): model specification + engine\n\n\n\n\n\n\n\nNote\n\n\n\nDocs:\n- Tidymodels homepage: https://www.tidymodels.org/\n- Getting started: https://www.tidymodels.org/start/\n- parsnip reference: https://parsnip.tidymodels.org/reference/index.html\n\n\nIn tidymodels, we separate:\n\n\nWhat kind of model we want → the specification\n\n\nHow it is implemented → the engine\n\n\n2.1 Model specification\nWe declare high-level models like this (classification / regression only, no engine yet):\n\n#R\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      4.0.0      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.2.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlog_spec   &lt;- logistic_reg(mode = \"classification\")\ntree_spec  &lt;- decision_tree(mode = \"classification\")\nrf_spec    &lt;- rand_forest(mode = \"regression\")\nknn_spec   &lt;- nearest_neighbor(mode = \"classification\")\n\nlog_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nComputational engine: rpart \n\n\nThis does not fit a model yet. It just says:\n\n“I want a logistic regression classifier”\n“I want a decision tree for classification”\netc.\n2.2 Engines: how the model is actually fit\nThe engine is the backend algorithm. We choose it with set_engine():\n\n#R\nlog_spec_glm &lt;- logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")      # uses base R glm() with binomial family\n\nlog_spec_glmnet &lt;- logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glmnet\")   # uses penalised regression (elastic net)\n\ntree_spec_rpart &lt;- decision_tree(mode = \"classification\") %&gt;%\n  set_engine(\"rpart\")    # uses rpart (CART)\n\nboost_spec_xgb &lt;- boost_tree(mode = \"classification\") %&gt;%\n  set_engine(\"xgboost\")  # uses xgboost library\n\nDifferent engines = different algorithms for the same kind of model.\n2.3 Fitting a logistic regression model with tidymodels\nBelow we use the Pima Indians diabetes data and compare to the glm() style you already know.\n\n#R\nlibrary(mlbench)\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  dplyr::select(glucose, mass, age, diabetes) %&gt;%\n  # we want \"pos\" (diabetes present) as the event of interest\n  dplyr::mutate(diabetes = forcats::fct_relevel(diabetes, \"pos\")) # here we are saying that what we are trying to predict (i=our positive, 1 class is \"pos\")\n\nsplit &lt;- initial_split(pima, prop = 0.8, strata = diabetes)\npima_train &lt;- training(split)\npima_test  &lt;- testing(split)\n\n# Model spec + engine\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")   # internally: glm(..., family = binomial)\n\nFit the model using fit() with a formula interface:\n\n#R\nlogistic_fit &lt;-\n  logistic_spec %&gt;%\n  fit(diabetes ~ glucose, data = pima_train)\n\nlogistic_fit\n\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetes ~ glucose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      glucose  \n    5.20800     -0.03666  \n\nDegrees of Freedom: 613 Total (i.e. Null);  612 Residual\nNull Deviance:      793.9 \nResidual Deviance: 654.4    AIC: 658.4\n\n\nInspect the coefficients (these are log-odds / logit coefficients):\n\n#R\ntidy(logistic_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   5.21     0.464        11.2 3.44e-29\n2 glucose      -0.0367   0.00358     -10.2 1.34e-24\n\n\n2.4 Making predictions in a consistent way\nAll parsnip models support the same predict() interface:\n\n#R\npima_preds &lt;-\n  predict(logistic_fit, new_data = pima_test, type = \"prob\") %&gt;%\n  dplyr::bind_cols(pima_test)\n\nhead(pima_preds)\n\n# A tibble: 6 × 6\n  .pred_pos .pred_neg glucose  mass   age diabetes\n      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   \n1    0.0872     0.913      78  31      26 pos     \n2    0.882      0.118     197  30.5    53 pos     \n3    0.176      0.824     100  30      32 pos     \n4    0.285      0.715     117  34.1    38 neg     \n5    0.138      0.862      92  19.9    28 neg     \n6    0.129      0.871      90  38.2    27 pos     \n\n\nWe can visualise predicted probability vs. glucose:\n\n#R\nlibrary(ggplot2)\n\nggplot(pima_preds, aes(x = glucose, y = .pred_pos)) +\n  geom_point(alpha = 0.4) +\n  labs(\n    x = \"Glucose\",\n    y = \"Predicted probability of diabetes (pos)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nHere:\n\n\n.pred_pos = predicted ( P( = ) )\n\n.pred_neg (not shown here) = predicted ( P() )\n2.5 Recipes + workflows (pipelines in tidymodels)\nTidymodels also includes tools for preprocessing and pipelines:\n\n\nrecipes → scaling, imputation, one-hot encoding, etc.\n\n\nworkflows → combine a model + recipe into a single object\n\n\n#R\nlog_recipe &lt;-\n  recipe(diabetes ~ glucose + age + mass, data = pima) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nlog_workflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_spec) %&gt;%\n  add_recipe(log_recipe)\n\nlog_fit_multi &lt;- fit(log_workflow, data = pima)\nlog_fit_multi\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  stats::glm(formula = ..y ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)      glucose          age         mass  \n     0.8506      -1.0395      -0.3547      -0.6433  \n\nDegrees of Freedom: 767 Total (i.e. Null);  764 Residual\nNull Deviance:      993.5 \nResidual Deviance: 755.7    AIC: 763.7\n\n\nNotice, when we open in our environment the model created, it includes too, the preprocessing information!\nThis is similar in spirit to scikit-learn pipelines.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#scikit-learn-python-unified-estimator-api",
    "href": "tidymodels_sklearn_overview.html#scikit-learn-python-unified-estimator-api",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "3. Scikit-learn (Python): unified estimator API",
    "text": "3. Scikit-learn (Python): unified estimator API\n\n\n\n\n\n\nNote\n\n\n\nDocs:\n- Scikit-learn homepage: https://scikit-learn.org/stable/\n- User guide: https://scikit-learn.org/stable/user_guide.html\n- API overview: https://scikit-learn.org/stable/developers/develop.html#apis-of-scikit-learn-objects\n\n\nIn scikit-learn, all models follow the same estimator API:\nmodel = SomeEstimator(hyperparameters)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nExamples:\n\n\nLinearRegression (ordinary least squares)\n\nLogisticRegression (classification)\nDecisionTreeClassifier\nRandomForestClassifier\n\nSVC (support vector classifier)\nKNeighborsClassifier\n\nMLPClassifier (neural network)\nPreprocessors like StandardScaler, OneHotEncoder\n\n\n3.1 A simple logistic regression in scikit-learn\n\n#Python\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ndata = load_breast_cancer()\nX = data.data\ny = data.target   # 0 = malignant, 1 = benign\n\n# Train / test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# Define model (specification)\nlog_reg = LogisticRegression(max_iter=1000,\n    solver=\"liblinear\")\n\n# Fit model\nlog_reg.fit(X_train, y_train)\n\n\n\n\nLogisticRegression(max_iter=1000, solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\n\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n# Predict\ny_pred = log_reg.predict(X_test)\n\nHere:\n\n\nLogisticRegression(...) = model specification (similar to logistic_reg() in parsnip)\nUnder the hood, it calls a numerical solver (e.g. \"lbfgs\", \"liblinear\", \"saga\") – this is analogous to an engine.\n3.2 Probabilities and log-odds (logits)\nWe can access predicted probabilities with predict_proba:\n\n#Python\ny_proba = log_reg.predict_proba(X_test)\ny_proba[:5]\n\narray([[7.09013048e-01, 2.90986952e-01],\n       [1.07801108e-02, 9.89219889e-01],\n       [6.67983197e-04, 9.99332017e-01],\n       [9.89039021e-01, 1.09609792e-02],\n       [2.77066904e-01, 7.22933096e-01]])\n\n\n\nColumn 0 = (P( x))\n\nColumn 1 = (P( x))\n\nThe underlying logits (log-odds) are related to these probabilities via the logistic function.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#pipelines-tidymodels-workflows-vs-scikit-learn-pipeline",
    "href": "tidymodels_sklearn_overview.html#pipelines-tidymodels-workflows-vs-scikit-learn-pipeline",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "4. Pipelines: tidymodels workflows vs scikit-learn Pipeline",
    "text": "4. Pipelines: tidymodels workflows vs scikit-learn Pipeline\nBoth ecosystems support pipelines to chain preprocessing + model in a leak-free way.\n4.1 Tidymodels workflow (R)\nWe already saw:\n\n#R\nlog_recipe &lt;-\n  recipe(diabetes ~ glucose + age + mass, data = pima) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")\n\nlog_workflow &lt;-\n  workflow() %&gt;%\n  add_model(logistic_spec) %&gt;%\n  add_recipe(log_recipe)\n\nlog_fit_multi &lt;- fit(log_workflow, data = pima)\n\nHere:\n\n\nrecipe = preprocessing steps\n\n\nlogistic_spec = model specification + engine\n\n\nworkflow = pipeline object (prep + fit together)\n4.2 Scikit-learn Pipeline (Python)\nThe scikit-learn equivalent uses Pipeline and transformers:\n\n#Python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),          # preprocessing step\n    (\"log_reg\", LogisticRegression(\n    max_iter=1000,\n    solver=\"liblinear\"\n))\n])\n\npipe.fit(X_train, y_train)\n\n\n\n\nPipeline(steps=[('scale', StandardScaler()),\n                ('log_reg',\n                 LogisticRegression(max_iter=1000, solver='liblinear'))])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nPipeline\n\n?Documentation for PipelineiFitted\n\n        \n            Parameters\n\n\nsteps \n[('scale', ...), ('log_reg', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n    \n\n\n\nStandardScaler\n?Documentation for StandardScaler\n        \n            Parameters\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n    \n\n\nLogisticRegression\n?Documentation for LogisticRegression\n        \n            Parameters\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n    \n\n\n\n\n\n\npipe.score(X_test, y_test)\n\n0.9883040935672515\n\n\n\n\nStandardScaler() = transformer (similar to step_normalize)\n\n\nLogisticRegression() = estimator (similar role to logistic_reg() + engine \"glm\")\n\n\nPipeline = links them, ensures the same transformations are applied in training and testing.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "tidymodels_sklearn_overview.html#summary-parsnip-tidymodels-vs-scikit-learn",
    "href": "tidymodels_sklearn_overview.html#summary-parsnip-tidymodels-vs-scikit-learn",
    "title": "(0) Unified Modelling Frameworks in R & Python",
    "section": "6. Summary: parsnip + tidymodels vs scikit-learn",
    "text": "6. Summary: parsnip + tidymodels vs scikit-learn\nBoth ecosystems share the same design philosophy:\n\n\nUnified interface for all models\n\nR: fit(), predict() on parsnip/workflow objects\n\nPython: fit(), predict() on estimators\n\n\n\nPipelines and tuning are built-in\n\nLeak-free preprocessing\n\n\n\nThis allows you to focus on the modelling ideas (classification, regression, evaluation, bias–variance, etc.) instead of constantly fighting with different syntaxes and one-off implementations.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(0) Unified Modelling Frameworks in R & Python"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html",
    "href": "logistic_regression_tidymodels.html",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "",
    "text": "Before fitting any model, we need to be clear about how probabilities, odds and log-odds (logits) are related.\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.2f\\n\", prob))\n\nProbability: 0.80\n\n# Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.2f\\n\", odds))\n\nOdds: 4.00\n\n# Convert odds to log-odds (logit)\nlog_odds &lt;- log(odds)\ncat(sprintf(\"Log-odds: %.2f\\n\", log_odds))\n\nLog-odds: 1.39\n\n\n\n\nProbability: chances of the event happening, between 0 and 1.\n\n\nOdds: ratio of probability of the event to probability of no event\n[ = ]\n\nLog-odds (logit): natural log of the odds\n[ (p) = () ]\n\nLog-odds are unbounded (from (-) to (+)) and this is what the logistic regression model is linear in.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#probabilities-odds-and-log-odds",
    "href": "logistic_regression_tidymodels.html#probabilities-odds-and-log-odds",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "",
    "text": "Before fitting any model, we need to be clear about how probabilities, odds and log-odds (logits) are related.\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.2f\\n\", prob))\n\nProbability: 0.80\n\n# Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.2f\\n\", odds))\n\nOdds: 4.00\n\n# Convert odds to log-odds (logit)\nlog_odds &lt;- log(odds)\ncat(sprintf(\"Log-odds: %.2f\\n\", log_odds))\n\nLog-odds: 1.39\n\n\n\n\nProbability: chances of the event happening, between 0 and 1.\n\n\nOdds: ratio of probability of the event to probability of no event\n[ = ]\n\nLog-odds (logit): natural log of the odds\n[ (p) = () ]\n\nLog-odds are unbounded (from (-) to (+)) and this is what the logistic regression model is linear in.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#data-pima-indians-diabetes",
    "href": "logistic_regression_tidymodels.html#data-pima-indians-diabetes",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "2. Data: Pima Indians Diabetes",
    "text": "2. Data: Pima Indians Diabetes\nWe will use the PimaIndiansDiabetes data from the mlbench package and keep only a few variables for now as in other exercises.\n\nlibrary(mlbench)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ infer        1.0.9      ✔ tune         2.0.1 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ recipes      1.3.1      ✔ yardstick    1.3.2 \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\ndata(\"PimaIndiansDiabetes\")\n\npima &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes, mass, age)\n\nglimpse(pima)\n\nRows: 768\nColumns: 4\n$ glucose  &lt;dbl&gt; 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139,…\n$ diabetes &lt;fct&gt; pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, n…\n$ mass     &lt;dbl&gt; 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37…\n$ age      &lt;dbl&gt; 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, 3…\n\n\n\n\nOutcome: diabetes (factor with levels neg and pos)\n\nPredictors: glucose, mass (BMI), age\n\n\nLet’s quickly check class balance:\n\npima %&gt;%\n  count(diabetes)\n\n  diabetes   n\n1      neg 500\n2      pos 268\n\n\nBy default, tidymodels will treat the first level of the factor as the “event of interest”.\nHere, \"neg\" comes first, but we want \"pos\" (diabetes present) to be the event.\n\npima &lt;- pima %&gt;%\n  mutate(diabetes = fct_relevel(diabetes, \"pos\"))\n\npima |&gt;\n  count(diabetes)\n\n  diabetes   n\n1      pos 268\n2      neg 500\n\n\nNow \"pos\" is the first level and will be the class for which probabilities are predicted.",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#logistic-regression-with-tidymodels-one-predictor",
    "href": "logistic_regression_tidymodels.html#logistic-regression-with-tidymodels-one-predictor",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "3. Logistic regression with tidymodels (one predictor)",
    "text": "3. Logistic regression with tidymodels (one predictor)\nWe start with a simple model: predict diabetes from glucose only.\n3.1 Specify the model\nWe use parsnip::logistic_reg() and set the engine to \"glm\" (which uses the classical GLM under the hood). Here is the link to\n\nlogistic_spec &lt;-\n  logistic_reg(mode = \"classification\") %&gt;%\n  set_engine(\"glm\")\n\nlogistic_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n3.2 Fit the model\nWe can fit the model directly with a formula, without recipes or workflows for now:\n\nlogistic_fit_glucose &lt;-\n  logistic_spec %&gt;%\n  fit(diabetes ~ glucose, data = pima)\n\nlogistic_fit_glucose\n\nparsnip model object\n\n\nCall:  stats::glm(formula = diabetes ~ glucose, family = stats::binomial, \n    data = data)\n\nCoefficients:\n(Intercept)      glucose  \n    5.35008     -0.03787  \n\nDegrees of Freedom: 767 Total (i.e. Null);  766 Residual\nNull Deviance:      993.5 \nResidual Deviance: 808.7    AIC: 812.7\n\n\nThe fitted model stores the estimated intercept and slope for the log-odds of diabetes given glucose.\nWe can see the underlying GLM coefficients:\n\ntidy(logistic_fit_glucose)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   5.35     0.421        12.7 4.99e-37\n2 glucose      -0.0379   0.00325     -11.6 2.38e-31\n\n\nThis corresponds to the model\n[ (P( = )) = _0 + _1 . ]\nTo get the probability for a given glucose value, we apply the logistic (sigmoid) function:\n[ p( = ) = . ]",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#from-model-to-odds-and-probabilities",
    "href": "logistic_regression_tidymodels.html#from-model-to-odds-and-probabilities",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "4. From model to odds and probabilities",
    "text": "4. From model to odds and probabilities\nLet’s compute the odds of having diabetes when glucose = 120 using the fitted model.\n\ncoefs &lt;- tidy(logistic_fit_glucose)\n\n#This is the same as in yesterday's practical doing logistic_fit_glucose[[\"fit]][[\"coefficients]]\nintercept &lt;- coefs$estimate[coefs$term == \"(Intercept)\"]\nslope     &lt;- coefs$estimate[coefs$term == \"glucose\"]\n\nglucose_value &lt;- 120\n\nlog_odds_120 &lt;- intercept + slope * glucose_value    # linear predictor z\nodds_120     &lt;- exp(log_odds_120)                    # odds\nprob_120     &lt;- 1 / (1 + exp(-log_odds_120))         # probability\n\ncat(\"Log-odds at glucose = 120:\", round(log_odds_120, 3), \"\\n\")\n\nLog-odds at glucose = 120: 0.805 \n\ncat(\"Odds at glucose = 120    :\", round(odds_120, 3), \"\\n\")\n\nOdds at glucose = 120    : 2.237 \n\ncat(\"Probability at glucose = 120:\", round(prob_120, 3), \"\\n\")\n\nProbability at glucose = 120: 0.691",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  },
  {
    "objectID": "logistic_regression_tidymodels.html#making-predictions-with-tidymodels",
    "href": "logistic_regression_tidymodels.html#making-predictions-with-tidymodels",
    "title": "(1a) Fitting a logistic regression model - R (tidymodels)",
    "section": "5. Making predictions with tidymodels",
    "text": "5. Making predictions with tidymodels\naugment() from broom/broom.mixed (loaded through tidymodels) makes it easy to attach predictions to the data.\n\npima_preds_glucose &lt;-\n  predict(logistic_fit_glucose, new_data = pima, type = \"prob\") %&gt;% # type = \"prob\" gives a column for each class probability\n  rename(prob_pos = .pred_pos, prob_neg = .pred_neg) %&gt;%\n  dplyr::bind_cols(pima)\n\npima_preds_glucose %&gt;%\n  select(glucose, diabetes, prob_pos, prob_neg) %&gt;%\n  head()\n\n# A tibble: 6 × 4\n  glucose diabetes prob_pos prob_neg\n    &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1     148 pos         0.563    0.437\n2      85 neg         0.106    0.894\n3     183 pos         0.829    0.171\n4      89 neg         0.121    0.879\n5     137 pos         0.460    0.540\n6     116 neg         0.278    0.722\n\n\n5.1 Visualising the fitted curve\nWe can now visualise how the predicted probability of diabetes changes with glucose.\n\nggplot(pima_preds_glucose, aes(x = glucose, y = prob_pos)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  labs(\n    x = \"Glucose\",\n    y = \"Predicted probability of diabetes (pos)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFor comparison, overlay the observed classes (pos / neg) as 0/1:\n\npima_preds_glucose %&gt;%\n  mutate(diabetes_numeric = if_else(diabetes == \"pos\", 1, 0)) |&gt;\n  ggplot(aes(x = glucose)) +\n  geom_point(aes(y = diabetes_numeric), alpha = 0.4) +\n  geom_line(aes(y = prob_pos), colour = \"red\", linewidth = 1) +\n  scale_y_continuous(breaks = c(0, 0.5, 1), limits = c(-0.05, 1.05)) +\n  labs(\n    x = \"Glucose\",\n    y = \"Observed class (0/1) and predicted probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nYou should see most pos cases (1) having higher predicted probabilities than neg cases (0).",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "(1a) Fitting a logistic regression model - R (tidymodels)"
    ]
  }
]