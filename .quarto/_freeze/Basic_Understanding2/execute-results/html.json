{
  "hash": "ec04d6622a34f10d794adfb4a587a31e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"(1a) Fitting a logistic regression model - R\"\nformat: html\n---\n\n\nFirst lets start with setting probabilities, odds and log odds and seeing how they are all related: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the probability of success\nprob <- 0.8\ncat(sprintf(\"Probability: %.1f\\n\", prob))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nProbability: 0.8\n```\n\n\n:::\n\n```{.r .cell-code}\n#Convert probability to odds\nodds <- prob / (1 - prob)\ncat(sprintf(\"Odds: %.1f\\n\", odds))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOdds: 4.0\n```\n\n\n:::\n\n```{.r .cell-code}\n#Convert odds to log-odds\nlogodds <- log(odds)\ncat(sprintf(\"Log-Odds: %.1f\\n\", logodds))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLog-Odds: 1.4\n```\n\n\n:::\n:::\n\nNow, lets fit a logistic regression model. As we said yesterday, the learning algorithm is already coded for us! So we just have to apply the correct function. In this case we will use `glm()` and set as input parameters `family = binomial` <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm>, <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family> which specifies implementing a logistic regression algorithm by setting the link function to __logit__ . We could have also used any of the models in `tidymodels` <https://parsnip.tidymodels.org/reference/logistic_reg.html>\n\nThe data we will be using is the same as yesterday's, `PimaIndiansDiabetes`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n```\n\n\n:::\n:::\n\nWe have lots of features/columns, so lets narrow it down to just a few \n\n\n::: {.cell}\n\n```{.r .cell-code}\nPimaIndiansDiabetes <- PimaIndiansDiabetes %>%\n  select(glucose, diabetes, mass, age)\n```\n:::\n\n\n- Outcome: \"diabetes\" is the target variable.Its a categorical varaible, made up of only two values, `neg` and `pos` \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(PimaIndiansDiabetes$diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nneg pos \n500 268 \n```\n\n\n:::\n:::\n\n\n\nFor our first logistic regression model, we are going to just use glucose as predictor only. We can specify this in the following way: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diabetes ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.350080   0.420827  -12.71   <2e-16 ***\nglucose      0.037873   0.003252   11.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nWhat is the predictive model we have fitted? \n\nThis one!\n\n$$\nP = \\frac{1}{1 + \\exp\\left(-\\left(-5.35 + 0.0379 \\cdot \\text{glucose}\\right)\\right)}\n$$\nWhere `glm()` has done all teh work for us, and identified the parameters $B_0$ and $B_1$ that best fit our dataset.These parameters are: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintercept <- coef(logistic_model)[1]\nslope <- coef(logistic_model)[2]\n\nintercept\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept) \n   -5.35008 \n```\n\n\n:::\n\n```{.r .cell-code}\nslope\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   glucose \n0.03787304 \n```\n\n\n:::\n:::\n\n\nWith this information, we could calcluate for example, the odds of having diabetes, if $glucose = 120$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#odds transformation\nodds <- exp(intercept + slope * 120) # Example: glucose = 120\ncat(\"Odds at glucose = 120:\", odds, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOdds at glucose = 120: 0.4469466 \n```\n\n\n:::\n:::\n\n\nJust as a side not, we want `pos` to be our predicted outcome, shpuld we  'dummify' the column so we have 0s and 1s only? Lets try and see:  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nPimaIndiansDiabetes$diabetes_dummy <- ifelse(PimaIndiansDiabetes$diabetes ==  \"pos\", 1, 0)\nhead(PimaIndiansDiabetes$diabetes_dummy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 0 1 0 1 0\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diabetes_dummy ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.350080   0.420827  -12.71   <2e-16 ***\nglucose      0.037873   0.003252   11.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n\nWe get the same results! glm recognizes this as a category and does it for us!\n\nNow, lets make predictions. We are going to use our same dataset we used to fit our data, and see how much the predicted probabilities differ from the real/true classes. Again, we use the function `predict`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- predict(logistic_model, type = \"response\")\n```\n:::\n\n\nLets add them to the dataset using `tidyverse` \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add predictions to the dataset\nPimaIndiansDiabetesWithPred <- PimaIndiansDiabetes %>%\n  mutate(predicted_probability = predictions)\n\n# Preview data with predictions\nhead(PimaIndiansDiabetesWithPred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  glucose diabetes mass age diabetes_dummy predicted_probability\n1     148      pos 33.6  50              1             0.5634385\n2      85      neg 26.6  31              0             0.1061322\n3     183      pos 23.3  32              1             0.8293015\n4      89      neg 28.1  21              0             0.1213849\n5     137      pos 43.1  33              1             0.4597189\n6     116      neg 25.6  30              0             0.2775177\n```\n\n\n:::\n:::\n\nWe do see a pattern right? Those with diabetes = `pos` (our 1) have higher probabilities. Lets double\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 4: Visualize results\nlibrary(ggplot2)\n\n# Plot glucose vs. predicted probability\nggplot(PimaIndiansDiabetesWithPred, aes(x = glucose, y = diabetes_dummy)) +\n  geom_point(alpha = 0.5) +\n  geom_point(aes(x = glucose, y = predicted_probability),alpha = 0.5, colour = \"red\") +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial), se = FALSE) +\n  labs(\n       x = \"Glucose\",\n       y = \"Diabetes\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Basic_Understanding2_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nNow build a logistic regression model with more features. Include `glucose + age + mass`. Compare predictions!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a logistic regression model\nlogistic_model_three <- glm(diabetes_dummy ~ glucose + age + mass, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model_three)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diabetes_dummy ~ glucose + age + mass, family = binomial, \n    data = PimaIndiansDiabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -8.393743   0.666067 -12.602  < 2e-16 ***\nglucose      0.032512   0.003329   9.767  < 2e-16 ***\nage          0.030157   0.007632   3.951 7.77e-05 ***\nmass         0.081590   0.013526   6.032 1.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 755.68  on 764  degrees of freedom\nAIC: 763.68\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\n#Continue!\n```\n:::\n\n\nNow lets do the same in python.",
    "supporting": [
      "Basic_Understanding2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}