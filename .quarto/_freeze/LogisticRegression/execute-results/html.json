{
  "hash": "f7af51e7272654e246eb84025832298f",
  "result": {
    "engine": "knitr",
    "markdown": "# Logistic Regression Practical\n\n## Classes PowerPoint Presentation\n\n<iframe src=\"logisticRegression_practical.pdf\" width=\"100%\" height=\"500\" frameborder=\"0\">\n\n</iframe>\n\n#\\| echo: false #\\| results: hide #\\| output: false #\\| include: false\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(caret) \ntheme_set(theme_bw())\n```\n:::\n\n\nLoad the data and remove NAs\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\nPimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)\n```\n:::\n\n\nInspect the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_n(PimaIndiansDiabetes2, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n531        2     122       60      18     106 29.8    0.717  22      neg\n108        4     144       58      28     140 29.5    0.287  37      neg\n306        2     120       76      37     105 39.7    0.215  29      neg\n```\n\n\n:::\n:::\n\n\nSplit the data into training and test set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntraining.samples <- PimaIndiansDiabetes2$diabetes %>% \n  createDataPartition(p = 0.8, list = FALSE)\ntrain.data  <- PimaIndiansDiabetes2[training.samples, ]\ntest.data <- PimaIndiansDiabetes2[-training.samples, ]\n```\n:::\n\n\nFit the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- glm( diabetes ~., data = train.data, family = binomial)\n```\n:::\n\n\nSummarize the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = diabetes ~ ., family = binomial, data = train.data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.053e+01  1.440e+00  -7.317 2.54e-13 ***\npregnant     1.005e-01  6.127e-02   1.640  0.10092    \nglucose      3.710e-02  6.486e-03   5.719 1.07e-08 ***\npressure    -3.876e-04  1.383e-02  -0.028  0.97764    \ntriceps      1.418e-02  1.998e-02   0.710  0.47800    \ninsulin      5.940e-04  1.508e-03   0.394  0.69371    \nmass         7.997e-02  3.180e-02   2.515  0.01190 *  \npedigree     1.329e+00  4.823e-01   2.756  0.00585 ** \nage          2.718e-02  2.020e-02   1.346  0.17840    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 398.80  on 313  degrees of freedom\nResidual deviance: 267.18  on 305  degrees of freedom\nAIC: 285.18\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nMake predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprobabilities <- model %>% predict(test.data, type = \"response\")\npredicted.classes <- ifelse(probabilities > 0.5, \"pos\", \"neg\")\n```\n:::\n\n\nModel accuracy\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(predicted.classes == test.data$diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7564103\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel <- glm( diabetes ~ glucose, data = train.data, family = binomial)\nsummary(model)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   z value     Pr(>|z|)\n(Intercept) -6.15882009 0.700096646 -8.797100 1.403974e-18\nglucose      0.04327234 0.005341133  8.101716 5.418949e-16\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- data.frame(glucose = c(20,  180))\nprobabilities <- model %>% predict(newdata, type = \"response\")\npredicted.classes <- ifelse(probabilities > 0.5, \"pos\", \"neg\")\npredicted.classes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    1     2 \n\"neg\" \"pos\" \n```\n\n\n:::\n\n```{.r .cell-code}\ntrain.data %>%\n  mutate(prob = ifelse(diabetes == \"pos\", 1, 0)) %>%\n  ggplot(aes(glucose, prob)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(\n    title = \"Logistic Regression Model\", \n    x = \"Plasma Glucose Concentration\",\n    y = \"Probability of being diabete-pos\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](LogisticRegression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmodel <- glm( diabetes ~ glucose + mass + pregnant, \n              data = train.data, family = binomial)\nsummary(model)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   z value     Pr(>|z|)\n(Intercept) -9.32369818 1.125997285 -8.280391 1.227711e-16\nglucose      0.03886154 0.005404219  7.190962 6.433636e-13\nmass         0.09458458 0.023529905  4.019760 5.825738e-05\npregnant     0.14466661 0.045125729  3.205857 1.346611e-03\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel <- glm( diabetes ~., data = train.data, family = binomial)\nsummary(model)$coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Estimate  Std. Error     z value     Pr(>|z|)\n(Intercept) -1.053400e+01 1.439679266 -7.31690975 2.537464e-13\npregnant     1.005031e-01 0.061266974  1.64041157 1.009196e-01\nglucose      3.709621e-02 0.006486093  5.71934633 1.069346e-08\npressure    -3.875933e-04 0.013826185 -0.02803328 9.776356e-01\ntriceps      1.417771e-02 0.019981885  0.70952823 4.779967e-01\ninsulin      5.939876e-04 0.001508231  0.39383055 6.937061e-01\nmass         7.997447e-02 0.031798907  2.51500698 1.190300e-02\npedigree     1.329149e+00 0.482291020  2.75590704 5.852963e-03\nage          2.718224e-02 0.020199295  1.34570257 1.783985e-01\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)      pregnant       glucose      pressure       triceps \n-1.053400e+01  1.005031e-01  3.709621e-02 -3.875933e-04  1.417771e-02 \n      insulin          mass      pedigree           age \n 5.939876e-04  7.997447e-02  1.329149e+00  2.718224e-02 \n```\n\n\n:::\n\n```{.r .cell-code}\nmodel <- glm( diabetes ~ pregnant + glucose + pressure + mass + pedigree, \n              data = train.data, family = binomial)\n\nprobabilities <- model %>% predict(test.data, type = \"response\")\nhead(probabilities)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       19        21        32        55        64        71 \n0.1352603 0.5127526 0.6795376 0.7517408 0.2734867 0.1648174 \n```\n\n\n:::\n\n```{.r .cell-code}\ncontrasts(test.data$diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    pos\nneg   0\npos   1\n```\n\n\n:::\n\n```{.r .cell-code}\npredicted.classes <- ifelse(probabilities > 0.5, \"pos\", \"neg\")\nhead(predicted.classes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   19    21    32    55    64    71 \n\"neg\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \n```\n\n\n:::\n\n```{.r .cell-code}\nmean(predicted.classes == test.data$diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7564103\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"mgcv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fit the model\ngam.model <- gam(diabetes ~ s(glucose) + mass + pregnant,\n                 data = train.data, family = \"binomial\")\n# Summarize model\nsummary(gam.model )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFamily: binomial \nLink function: logit \n\nFormula:\ndiabetes ~ s(glucose) + mass + pregnant\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.59794    0.86982  -5.286 1.25e-07 ***\nmass         0.09458    0.02353   4.020 5.83e-05 ***\npregnant     0.14467    0.04513   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df Chi.sq p-value    \ns(glucose)   1      1  51.71  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.339   Deviance explained = 29.8%\nUBRE = -0.083171  Scale est. = 1         n = 314\n```\n\n\n:::\n\n```{.r .cell-code}\n# Make predictions\nprobabilities <- gam.model %>% predict(test.data, type = \"response\")\npredicted.classes <- ifelse(probabilities> 0.5, \"pos\", \"neg\")\n# Model Accuracy\nmean(predicted.classes == test.data$diabetes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7820513\n```\n\n\n:::\n:::\n\n\n## optional ========================================================================\n\nExample implementation from scratch:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nN <- 200 # number of points per class\nD <- 2 # dimensionality, we use 2D data for easy visualization\nK <- 2 # number of classes, binary for logistic regression\nX <- data.frame() # data matrix (each row = single example, can view as xy coordinates)\ny <- data.frame() # class labels\n\nset.seed(56)\n\nt <- seq(0,1,length.out = N) \nfor (j in (1:K)){\n  # t, m are parameters of parametric equations x1, x2\n  # t <- seq(0,1,length.out = N) \n  # add randomness \n  m <- rnorm(N, j+0.5, 0.25) \n  Xtemp <- data.frame(x1 = 3*t , x2 = m - t) \n  ytemp <- data.frame(matrix(j-1, N, 1))\n  X <- rbind(X, Xtemp)\n  y <- rbind(y, ytemp)\n}\n\ndata <- cbind(X,y)\ncolnames(data) <- c(colnames(X), 'label')\n\n# create dir images\n#dir.create(file.path('.', 'images'), showWarnings = FALSE)\n\n# lets visualize the data:\ndata_plot <- ggplot(data) + geom_point(aes(x=x1, y=x2, color = as.character(label)), size = 2) + \n  scale_colour_discrete(name  =\"Label\") + \n  ylim(0, 3) + coord_fixed(ratio = 1) +\n  ggtitle('Data to be classified') +\n  theme_bw(base_size = 12) +\n  theme(legend.position=c(0.85, 0.87))\n\n#png(file.path('images', 'data_plot.png'))\nprint(data_plot)\n```\n\n::: {.cell-output-display}\n![](LogisticRegression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#dev.off()\n\n#sigmoid function, inverse of logit\nsigmoid <- function(z){1/(1+exp(-z))}\n\n#cost function\ncost <- function(theta, X, y){\n  m <- length(y) # number of training examples\n  h <- sigmoid(X %*% theta)\n  J <- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m\n  J\n}\n\n#gradient function\ngrad <- function(theta, X, y){\n  m <- length(y) \n  \n  h <- sigmoid(X%*%theta)\n  grad <- (t(X)%*%(h - y))/m\n  grad\n}\n\nlogisticReg <- function(X, y){\n  #remove NA rows\n  X <- na.omit(X)\n  y <- na.omit(y)\n  #add bias term and convert to matrix\n  X <- mutate(X, bias =1)\n  #move the bias column to col1\n  X <- as.matrix(X[, c(ncol(X), 1:(ncol(X)-1))])\n  y <- as.matrix(y)\n  #initialize theta\n  theta <- matrix(rep(0, ncol(X)), nrow = ncol(X))\n  #use the optim function to perform gradient descent\n  costOpti <- optim(theta, fn = cost, gr = grad, X=X, y=y)\n  #return coefficients\n  return(costOpti$par)\n}\n\nlogisticProb <- function(theta, X){\n  X <- na.omit(X)\n  #add bias term and convert to matrix\n  X <- mutate(X, bias =1)\n  X <- as.matrix(X[,c(ncol(X), 1:(ncol(X)-1))])\n  return(sigmoid(X%*%theta))\n}\n\nlogisticPred <- function(prob){\n  return(round(prob, 0))\n}\n\n# training\ntheta <- logisticReg(X, y)\nprob <- logisticProb(theta, X)\npred <- logisticPred(prob)\n\n# generate a grid for decision boundary, this is the test set\ngrid <- expand.grid(seq(0, 3, length.out = 100), seq(0, 3, length.out = 100))\n# predict the probability\nprobZ <- logisticProb(theta, grid)\n# predict the label\nZ <- logisticPred(probZ)\ngridPred = cbind(grid, Z)\n\n# decision boundary visualization\np <- ggplot() + geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) + \n  geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F)+ \n  ylim(0, 3) +\n  ggtitle('Decision Boundary for Logistic Regression') +\n  coord_fixed(ratio = 1) +\n  theme_bw(base_size = 12) \n\n#png(file.path('images', 'logistic_regression.png'))\nprint(p)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 200 rows containing missing values or values outside the scale range\n(`geom_tile()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](LogisticRegression_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\n#dev.off()\n```\n:::\n\n",
    "supporting": [
      "LogisticRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}