{"title":"(1a) Fitting a logistic regression model - R","markdown":{"yaml":{"title":"(1a) Fitting a logistic regression model - R","format":"html"},"headingText":"Define the probability of success","containsRefs":false,"markdown":"\n\n\nFirst lets start with setting probabilities, odds and log odds and seeing how they are all related: \n\n```{r}\nprob <- 0.8\ncat(sprintf(\"Probability: %.1f\\n\", prob))\n\n#Convert probability to odds\nodds <- prob / (1 - prob)\ncat(sprintf(\"Odds: %.1f\\n\", odds))\n\n#Convert odds to log-odds\nlogodds <- log(odds)\ncat(sprintf(\"Log-Odds: %.1f\\n\", logodds))\n\n\n```\nNow, lets fit a logistic regression model. As we said yesterday, the learning algorithm is already coded for us! So we just have to apply the correct function. In this case we will use `glm()` and set as input parameters `family = binomial` <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm>, <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family> which specifies implementing a logistic regression algorithm by setting the link function to __logit__ . We could have also used any of the models in `tidymodels` <https://parsnip.tidymodels.org/reference/logistic_reg.html>\n\nThe data we will be using is the same as yesterday's, `PimaIndiansDiabetes`\n\n```{r}\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n```\n\n\n```{r}\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n```\nWe have lots of features/columns, so lets narrow it down to just a few \n\n```{r}\nPimaIndiansDiabetes <- PimaIndiansDiabetes %>%\n  select(glucose, diabetes, mass, age)\n```\n\n- Outcome: \"diabetes\" is the target variable.Its a categorical varaible, made up of only two values, `neg` and `pos` \n\n```{r}\ntable(PimaIndiansDiabetes$diabetes)\n```\n\n\nFor our first logistic regression model, we are going to just use glucose as predictor only. We can specify this in the following way: \n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\nWhat is the predictive model we have fitted? \n\nThis one!\n\n$$\nP = \\frac{1}{1 + \\exp\\left(-\\left(-5.35 + 0.0379 \\cdot \\text{glucose}\\right)\\right)}\n$$\nWhere `glm()` has done all teh work for us, and identified the parameters $B_0$ and $B_1$ that best fit our dataset.These parameters are: \n\n\n```{r}\nintercept <- coef(logistic_model)[1]\nslope <- coef(logistic_model)[2]\n\nintercept\nslope\n```\n\nWith this information, we could calcluate for example, the odds of having diabetes, if $glucose = 120$\n\n```{r}\n#odds transformation\nodds <- exp(intercept + slope * 120) # Example: glucose = 120\ncat(\"Odds at glucose = 120:\", odds, \"\\n\")\n```\n\nJust as a side not, we want `pos` to be our predicted outcome, shpuld we  'dummify' the column so we have 0s and 1s only? Lets try and see:  \n\n```{r}\nPimaIndiansDiabetes$diabetes_dummy <- ifelse(PimaIndiansDiabetes$diabetes ==  \"pos\", 1, 0)\nhead(PimaIndiansDiabetes$diabetes_dummy)\n```\n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\n\nWe get the same results! glm recognizes this as a category and does it for us!\n\nNow, lets make predictions. We are going to use our same dataset we used to fit our data, and see how much the predicted probabilities differ from the real/true classes. Again, we use the function `predict`.\n\n```{r}\npredictions <- predict(logistic_model, type = \"response\")\n```\n\nLets add them to the dataset using `tidyverse` \n\n```{r}\n# Add predictions to the dataset\nPimaIndiansDiabetesWithPred <- PimaIndiansDiabetes %>%\n  mutate(predicted_probability = predictions)\n\n# Preview data with predictions\nhead(PimaIndiansDiabetesWithPred)\n```\nWe do see a pattern right? Those with diabetes = `pos` (our 1) have higher probabilities. Lets double\n\n```{r}\n# Step 4: Visualize results\nlibrary(ggplot2)\n\n# Plot glucose vs. predicted probability\nggplot(PimaIndiansDiabetesWithPred, aes(x = glucose, y = diabetes_dummy)) +\n  geom_point(alpha = 0.5) +\n  geom_point(aes(x = glucose, y = predicted_probability),alpha = 0.5, colour = \"red\") +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial), se = FALSE) +\n  labs(\n       x = \"Glucose\",\n       y = \"Diabetes\") +\n  theme_minimal()\n\n```\nNow build a logistic regression model with more features. Include `glucose + age + mass`. Compare predictions!\n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model_three <- glm(diabetes_dummy ~ glucose + age + mass, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model_three)\n\n#Continue!\n\n\n```\n\nNow lets do the same in python.","srcMarkdownNoYaml":"\n\n\nFirst lets start with setting probabilities, odds and log odds and seeing how they are all related: \n\n```{r}\n# Define the probability of success\nprob <- 0.8\ncat(sprintf(\"Probability: %.1f\\n\", prob))\n\n#Convert probability to odds\nodds <- prob / (1 - prob)\ncat(sprintf(\"Odds: %.1f\\n\", odds))\n\n#Convert odds to log-odds\nlogodds <- log(odds)\ncat(sprintf(\"Log-Odds: %.1f\\n\", logodds))\n\n\n```\nNow, lets fit a logistic regression model. As we said yesterday, the learning algorithm is already coded for us! So we just have to apply the correct function. In this case we will use `glm()` and set as input parameters `family = binomial` <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm>, <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family> which specifies implementing a logistic regression algorithm by setting the link function to __logit__ . We could have also used any of the models in `tidymodels` <https://parsnip.tidymodels.org/reference/logistic_reg.html>\n\nThe data we will be using is the same as yesterday's, `PimaIndiansDiabetes`\n\n```{r}\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n```\n\n\n```{r}\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n```\nWe have lots of features/columns, so lets narrow it down to just a few \n\n```{r}\nPimaIndiansDiabetes <- PimaIndiansDiabetes %>%\n  select(glucose, diabetes, mass, age)\n```\n\n- Outcome: \"diabetes\" is the target variable.Its a categorical varaible, made up of only two values, `neg` and `pos` \n\n```{r}\ntable(PimaIndiansDiabetes$diabetes)\n```\n\n\nFor our first logistic regression model, we are going to just use glucose as predictor only. We can specify this in the following way: \n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\nWhat is the predictive model we have fitted? \n\nThis one!\n\n$$\nP = \\frac{1}{1 + \\exp\\left(-\\left(-5.35 + 0.0379 \\cdot \\text{glucose}\\right)\\right)}\n$$\nWhere `glm()` has done all teh work for us, and identified the parameters $B_0$ and $B_1$ that best fit our dataset.These parameters are: \n\n\n```{r}\nintercept <- coef(logistic_model)[1]\nslope <- coef(logistic_model)[2]\n\nintercept\nslope\n```\n\nWith this information, we could calcluate for example, the odds of having diabetes, if $glucose = 120$\n\n```{r}\n#odds transformation\nodds <- exp(intercept + slope * 120) # Example: glucose = 120\ncat(\"Odds at glucose = 120:\", odds, \"\\n\")\n```\n\nJust as a side not, we want `pos` to be our predicted outcome, shpuld we  'dummify' the column so we have 0s and 1s only? Lets try and see:  \n\n```{r}\nPimaIndiansDiabetes$diabetes_dummy <- ifelse(PimaIndiansDiabetes$diabetes ==  \"pos\", 1, 0)\nhead(PimaIndiansDiabetes$diabetes_dummy)\n```\n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model <- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n```\n\n\nWe get the same results! glm recognizes this as a category and does it for us!\n\nNow, lets make predictions. We are going to use our same dataset we used to fit our data, and see how much the predicted probabilities differ from the real/true classes. Again, we use the function `predict`.\n\n```{r}\npredictions <- predict(logistic_model, type = \"response\")\n```\n\nLets add them to the dataset using `tidyverse` \n\n```{r}\n# Add predictions to the dataset\nPimaIndiansDiabetesWithPred <- PimaIndiansDiabetes %>%\n  mutate(predicted_probability = predictions)\n\n# Preview data with predictions\nhead(PimaIndiansDiabetesWithPred)\n```\nWe do see a pattern right? Those with diabetes = `pos` (our 1) have higher probabilities. Lets double\n\n```{r}\n# Step 4: Visualize results\nlibrary(ggplot2)\n\n# Plot glucose vs. predicted probability\nggplot(PimaIndiansDiabetesWithPred, aes(x = glucose, y = diabetes_dummy)) +\n  geom_point(alpha = 0.5) +\n  geom_point(aes(x = glucose, y = predicted_probability),alpha = 0.5, colour = \"red\") +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial), se = FALSE) +\n  labs(\n       x = \"Glucose\",\n       y = \"Diabetes\") +\n  theme_minimal()\n\n```\nNow build a logistic regression model with more features. Include `glucose + age + mass`. Compare predictions!\n\n```{r}\n\n# Fit a logistic regression model\nlogistic_model_three <- glm(diabetes_dummy ~ glucose + age + mass, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model_three)\n\n#Continue!\n\n\n```\n\nNow lets do the same in python."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"python":{"version":"r-reticulate"},"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Basic_Understanding2.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","theme":"cosmo","title":"(1a) Fitting a logistic regression model - R"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}