[
  {
    "objectID": "Basic_Understanding2.html",
    "href": "Basic_Understanding2.html",
    "title": "(1a) Fitting a logistic regression model - R",
    "section": "",
    "text": "First lets start with setting probabilities, odds and log odds and seeing how they are all related:\n\n# Define the probability of success\nprob &lt;- 0.8\ncat(sprintf(\"Probability: %.1f\\n\", prob))\n\nProbability: 0.8\n\n#Convert probability to odds\nodds &lt;- prob / (1 - prob)\ncat(sprintf(\"Odds: %.1f\\n\", odds))\n\nOdds: 4.0\n\n#Convert odds to log-odds\nlogodds &lt;- log(odds)\ncat(sprintf(\"Log-Odds: %.1f\\n\", logodds))\n\nLog-Odds: 1.4\n\n\nNow, lets fit a logistic regression model. As we said yesterday, the learning algorithm is already coded for us! So we just have to apply the correct function. In this case we will use glm() and set as input parameters family = binomial https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm, https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family which specifies implementing a logistic regression algorithm by setting the link function to logit . We could have also used any of the models in tidymodels https://parsnip.tidymodels.org/reference/logistic_reg.html\nThe data we will be using is the same as yesterday’s, PimaIndiansDiabetes\n\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ndata(\"PimaIndiansDiabetes\")\nhead(PimaIndiansDiabetes)\n\n  pregnant glucose pressure triceps insulin mass pedigree age diabetes\n1        6     148       72      35       0 33.6    0.627  50      pos\n2        1      85       66      29       0 26.6    0.351  31      neg\n3        8     183       64       0       0 23.3    0.672  32      pos\n4        1      89       66      23      94 28.1    0.167  21      neg\n5        0     137       40      35     168 43.1    2.288  33      pos\n6        5     116       74       0       0 25.6    0.201  30      neg\n\n\nWe have lots of features/columns, so lets narrow it down to just a few\n\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes, mass, age)\n\n\nOutcome: “diabetes” is the target variable.Its a categorical varaible, made up of only two values, neg and pos\n\n\n\ntable(PimaIndiansDiabetes$diabetes)\n\n\nneg pos \n500 268 \n\n\nFor our first logistic regression model, we are going to just use glucose as predictor only. We can specify this in the following way:\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n\n\nCall:\nglm(formula = diabetes ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1096  -0.7837  -0.5365   0.8566   3.2726  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.350080   0.420827  -12.71   &lt;2e-16 ***\nglucose      0.037873   0.003252   11.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n\n\nWhat is the predictive model we have fitted?\nThis one!\n\\[\nP = \\frac{1}{1 + \\exp\\left(-\\left(-5.35 + 0.0379 \\cdot \\text{glucose}\\right)\\right)}\n\\] Where glm() has done all teh work for us, and identified the parameters \\(B_0\\) and \\(B_1\\) that best fit our dataset.These parameters are:\n\nintercept &lt;- coef(logistic_model)[1]\nslope &lt;- coef(logistic_model)[2]\n\nintercept\n\n(Intercept) \n   -5.35008 \n\nslope\n\n   glucose \n0.03787304 \n\n\nWith this information, we could calcluate for example, the odds of having diabetes, if \\(glucose = 120\\)\n\n#odds transformation\nodds &lt;- exp(intercept + slope * 120) # Example: glucose = 120\ncat(\"Odds at glucose = 120:\", odds, \"\\n\")\n\nOdds at glucose = 120: 0.4469466 \n\n\nJust as a side not, we want pos to be our predicted outcome, shpuld we ‘dummify’ the column so we have 0s and 1s only? Lets try and see:\n\nPimaIndiansDiabetes$diabetes_dummy &lt;- ifelse(PimaIndiansDiabetes$diabetes ==  \"pos\", 1, 0)\nhead(PimaIndiansDiabetes$diabetes_dummy)\n\n[1] 1 0 1 0 1 0\n\n\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model)\n\n\nCall:\nglm(formula = diabetes_dummy ~ glucose, family = binomial, data = PimaIndiansDiabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1096  -0.7837  -0.5365   0.8566   3.2726  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.350080   0.420827  -12.71   &lt;2e-16 ***\nglucose      0.037873   0.003252   11.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 808.72  on 766  degrees of freedom\nAIC: 812.72\n\nNumber of Fisher Scoring iterations: 4\n\n\nWe get the same results! glm recognizes this as a category and does it for us!\nNow, lets make predictions. We are going to use our same dataset we used to fit our data, and see how much the predicted probabilities differ from the real/true classes. Again, we use the function predict.\n\npredictions &lt;- predict(logistic_model, type = \"response\")\n\nLets add them to the dataset using tidyverse\n\n# Add predictions to the dataset\nPimaIndiansDiabetesWithPred &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(predicted_probability = predictions)\n\n# Preview data with predictions\nhead(PimaIndiansDiabetesWithPred)\n\n  glucose diabetes mass age diabetes_dummy predicted_probability\n1     148      pos 33.6  50              1             0.5634385\n2      85      neg 26.6  31              0             0.1061322\n3     183      pos 23.3  32              1             0.8293015\n4      89      neg 28.1  21              0             0.1213849\n5     137      pos 43.1  33              1             0.4597189\n6     116      neg 25.6  30              0             0.2775177\n\n\nWe do see a pattern right? Those with diabetes = pos (our 1) have higher probabilities. Lets double\n\n# Step 4: Visualize results\nlibrary(ggplot2)\n\n# Plot glucose vs. predicted probability\nggplot(PimaIndiansDiabetesWithPred, aes(x = glucose, y = diabetes_dummy)) +\n  geom_point(alpha = 0.5) +\n  geom_point(aes(x = glucose, y = predicted_probability),alpha = 0.5, colour = \"red\") +\n  geom_smooth(method = \"glm\", method.args = list(family = binomial), se = FALSE) +\n  labs(\n       x = \"Glucose\",\n       y = \"Diabetes\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNow build a logistic regression model with more features. Include glucose + age + mass. Compare predictions!\n\n# Fit a logistic regression model\nlogistic_model_three &lt;- glm(diabetes_dummy ~ glucose + age + mass, data = PimaIndiansDiabetes, family = binomial)\n\n# View model summary\nsummary(logistic_model_three)\n\n\nCall:\nglm(formula = diabetes_dummy ~ glucose + age + mass, family = binomial, \n    data = PimaIndiansDiabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3809  -0.7476  -0.4357   0.7861   2.8263  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.393743   0.666067 -12.602  &lt; 2e-16 ***\nglucose      0.032512   0.003329   9.767  &lt; 2e-16 ***\nage          0.030157   0.007632   3.951 7.77e-05 ***\nmass         0.081590   0.013526   6.032 1.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 993.48  on 767  degrees of freedom\nResidual deviance: 755.68  on 764  degrees of freedom\nAIC: 763.68\n\nNumber of Fisher Scoring iterations: 5\n\n#Continue!\n\nNow lets do the same in python.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Basic_Understanding_Python.html",
    "href": "Basic_Understanding_Python.html",
    "title": "(1b) Fitting a logistic regression model - Python",
    "section": "",
    "text": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\n\n\n# Load the Pima Indians Diabetes dataset\ndata = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n\n/Users/bravol/Library/r-miniconda/envs/r-reticulate/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n\nPimaIndiansDiabetes = data.frame\n\nWe have some different names (plas is glucose in R), but same idea.\n\n# Select relevant features and target variable\nPimaIndiansDiabetes = PimaIndiansDiabetes[[\"plas\", \"class\", \"mass\", \"age\"]]\n\n\n# Define predictor (X) and target (y) variables\nX = PimaIndiansDiabetes[[\"plas\"]]\ny = PimaIndiansDiabetes[\"class\"]\n\n\n# Fit a logistic regression model using sklearn\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegressionLogisticRegression()\n\n\n\n\n\n# Display model coefficients\nintercept = logistic_model.intercept_[0]\nslope = logistic_model.coef_[0][0]\nprint(f\"Intercept: {intercept}, Slope: {slope}\")\n\nIntercept: -5.350028074882648, Slope: 0.037872619821980175\n\n\n\n# Predict probabilities\nPimaIndiansDiabetes[\"predicted_probability\"] = logistic_model.predict_proba(X)[:, 1]\n\n&lt;string&gt;:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\nPimaIndiansDiabetes.head()\n\n    plas            class  mass   age  predicted_probability\n0  148.0  tested_positive  33.6  50.0               0.563436\n1   85.0  tested_negative  26.6  31.0               0.106134\n2  183.0  tested_positive  23.3  32.0               0.829298\n3   89.0  tested_negative  28.1  21.0               0.121387\n4  137.0  tested_positive  43.1  33.0               0.459718\n\n\n\n# Visualise predictions\nplt.figure(figsize=(8, 6))\nplt.scatter(PimaIndiansDiabetes[\"plas\"], PimaIndiansDiabetes[\"class\"], alpha=0.5, label=\"Actual\")\nplt.scatter(PimaIndiansDiabetes[\"plas\"], PimaIndiansDiabetes[\"predicted_probability\"], color=\"red\", alpha=0.5, label=\"Predicted\")\nplt.plot(\n    np.sort(PimaIndiansDiabetes[\"plas\"]),\n    np.sort(logistic_model.predict_proba(X)[:, 1]),\n    color=\"blue\",\n    label=\"Logistic Regression Curve\"\n)\nplt.xlabel(\"Glucose\")\nplt.ylabel(\"Diabetes (0 or 1)\")\nplt.title(\"Logistic Regression: Glucose vs Diabetes\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\nNow include more predictors: “plas” and “age”\n\n# Fit logistic regression with multiple predictors\nX_multi = PimaIndiansDiabetes[[\"plas\", \"age\"]]\nlogistic_model_two = LogisticRegression()\nlogistic_model_two.fit(X_multi, y)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegressionLogisticRegression()\n\n\n\n\n\n# Display model coefficients\nintercept = logistic_model_two.intercept_[0]\nslope = logistic_model_two.coef_[0][0]\nprint(f\"Intercept: {intercept}, Slope: {slope}\")\n\nIntercept: -5.912369223097624, Slope: 0.03564371909912629\n\n\n\n# Predict probabilities for the multi-feature model\nPimaIndiansDiabetes[\"predicted_probability_two\"] = logistic_model_two.predict_proba(X_multi)[:, 1]\n\n&lt;string&gt;:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\nVisualize the probabilities\n\nPimaIndiansDiabetes.head()\n\n    plas            class  ...  predicted_probability  predicted_probability_two\n0  148.0  tested_positive  ...               0.563436                   0.646059\n1   85.0  tested_negative  ...               0.106134                   0.107690\n2  183.0  tested_positive  ...               0.829298                   0.802707\n3   89.0  tested_negative  ...               0.121387                   0.097990\n4  137.0  tested_positive  ...               0.459718                   0.447313\n\n[5 rows x 6 columns]\n\n\n\nAdd mass to the modeling too (so in total three predictors, one outcome variable), how do the predictions change?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "DecisionThreshold.html",
    "href": "DecisionThreshold.html",
    "title": "2 Decision threshold",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# Load the Pima Indians Diabetes dataset\ndata(\"PimaIndiansDiabetes\")\n\n\n# Narrow down the dataset\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes)\n\n# Convert the outcome variable to binary (0 = neg, 1 = pos)\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(diabetes_dummy = ifelse(diabetes == \"pos\", 1, 0))\n\n# Fit a logistic regression model\nlogistic_model &lt;- glm(diabetes_dummy ~ glucose, data = PimaIndiansDiabetes, family = binomial)\n\n# Predict probabilities\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(predicted_probability = predict(logistic_model, type = \"response\"))\n\n\n# Create classifications for different thresholds\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  mutate(\n    class_0_5 = ifelse(predicted_probability &gt;= 0.5, 1, 0),\n    class_0_3 = ifelse(predicted_probability &gt;= 0.3, 1, 0),\n    class_0_7 = ifelse(predicted_probability &gt;= 0.7, 1, 0)\n  )\n\nWhat is hapenning when we increase the threshold? See how the number of correct (true y and predicted y) change!\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_3)\n\n   \n      0   1\n  0 331 169\n  1  66 202\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_5)\n\n   \n      0   1\n  0 443  57\n  1 138 130\n\n\n\ntable(PimaIndiansDiabetes$diabetes_dummy, PimaIndiansDiabetes$class_0_7)\n\n   \n      0   1\n  0 484  16\n  1 195  73\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Gradients.html",
    "href": "Gradients.html",
    "title": "3 Gradients",
    "section": "",
    "text": "How are we finding the model that best fits the data? Through optimization! Here we calculate the parameters B0 and B1 using gradient descent of log-loss, gradient ascent of log likelihood and compare it with the function glm that already does this for us.\n\n# Load necessary libraries\nlibrary(mlbench)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Load the Pima Indians Diabetes dataset\ndata(\"PimaIndiansDiabetes\")\nPimaIndiansDiabetes &lt;- PimaIndiansDiabetes %&gt;%\n  select(glucose, diabetes) %&gt;%\n  mutate(diabetes_dummy = ifelse(diabetes == \"pos\", 1, 0))\n\n\n# Data preparation\nx &lt;- PimaIndiansDiabetes$glucose\ny &lt;- PimaIndiansDiabetes$diabetes_dummy\n\n\n# Sigmoid function\nsigmoid &lt;- function(z) {\n  1 / (1 + exp(-z))\n}\n\n\n# Log-loss function (for gradient descent) - coded equation from slides\nlog_loss &lt;- function(beta_0, beta_1, x, y) {\n  z &lt;- beta_0 + beta_1 * x\n  y_pred &lt;- sigmoid(z)\n  -sum(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n}\n\n\n# Log-likelihood function (for gradient ascent)\nlog_likelihood &lt;- function(beta_0, beta_1, x, y) {\n  z &lt;- beta_0 + beta_1 * x\n  y_pred &lt;- sigmoid(z)\n  sum(y * log(y_pred) + (1 - y) * log(1 - y_pred))\n}\n\nTo plot - do not need to know this!\n\n# Generate data\nset.seed(123)\n#x &lt;- seq(50, 200, length.out = 100)  # glucose values\n#y_dummy &lt;- sample(0:1, length(x), replace = TRUE)  # random dummy diabetes values\n\n# Generate a grid of beta_0 and beta_1 values\nbeta_0_values &lt;- seq(-10, 10, length.out = 50)\nbeta_1_values &lt;- seq(-1, 1, length.out = 50)\n\n# Compute log-loss and log-likelihood values\nlog_loss_matrix &lt;- outer(beta_0_values, beta_1_values, Vectorize(function(b0, b1) log_loss(b0, b1, x, y)))\nlog_likelihood_matrix &lt;- outer(beta_0_values, beta_1_values, Vectorize(function(b0, b1) log_likelihood(b0, b1, x, y)))\n\n# Plot the log-loss function\nlog_loss_plot &lt;- plot_ly(\n  x = beta_0_values, \n  y = beta_1_values, \n  z = log_loss_matrix, \n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"Log-Loss Function\",\n    scene = list(\n      xaxis = list(title = \"Beta 0\"),\n      yaxis = list(title = \"Beta 1\"),\n      zaxis = list(title = \"Log-Loss\")\n    )\n)\n\n# Plot the log-likelihood function\nlog_likelihood_plot &lt;- plot_ly(\n  x = beta_0_values, \n  y = beta_1_values, \n  z = log_likelihood_matrix, \n  type = \"surface\"\n) %&gt;%\n  layout(\n    title = \"Log-Likelihood Function\",\n    scene = list(\n      xaxis = list(title = \"Beta 0\"),\n      yaxis = list(title = \"Beta 1\"),\n      zaxis = list(title = \"Log-Likelihood\")\n    )\n)\n\n# Display the plots\nlog_loss_plot\n\n\n\n\nlog_likelihood_plot\n\n\n\n\n\n\n# Gradient descent implementation\ngradient_descent &lt;- function(x, y, learning_rate, iterations) { # we can play with these two!\n  store_beta_0 &lt;- NULL\n  store_beta_1 &lt;- NULL\n  beta_0 &lt;- -5  # Initialize intercept #random points! Can play around with these ones too\n  beta_1 &lt;- 1  # Initialize slope #random points! Can play around with these ones too\n  for (i in 1:iterations) {\n    z &lt;- beta_0 + beta_1 * x #linear regression function/logit\n    y_pred &lt;- sigmoid(z) #logistic regression\n    grad_beta_0 &lt;- mean(y_pred - y) #partial derivative of intercept\n    grad_beta_1 &lt;- mean((y_pred - y) * x) #partial derivative of slope\n    beta_0 &lt;- beta_0 - learning_rate * grad_beta_0 #gradient descent equation! Updating the points\n    beta_1 &lt;- beta_1 - learning_rate * grad_beta_1\n    store_beta_0[i] &lt;- beta_0\n    store_beta_1[i] &lt;- beta_1\n  }\n  data.frame(beta_1 = store_beta_1, beta_0 = store_beta_0)\n}\n\n\ngd_result &lt;- gradient_descent(x, y, learning_rate = 0.01, iterations = 10)\n#View(gd_result)\n\nMinimum point found after 1000 iterations:\n\npaste0(\"Glucose/Slope:, \", gd_result$beta_1[dim(gd_result)[2]])\n\n[1] \"Glucose/Slope:, -0.432030882563657\"\n\npaste0(\"Intercept: \", gd_result$beta_0[dim(gd_result)[2]])\n\n[1] \"Intercept: -5.01289148560383\"\n\n\n\n# Gradient ascent implementation\ngradient_ascent &lt;- function(x, y, learning_rate, iterations) {\n  store_beta_0 &lt;- NULL\n  store_beta_1 &lt;- NULL\n  beta_0 &lt;- 0  # Initialize intercept\n  beta_1 &lt;- 0  # Initialize slope\n  for (i in 1:iterations) {\n    z &lt;- beta_0 + beta_1 * x\n    y_pred &lt;- sigmoid(z)\n    grad_beta_0 &lt;- mean(y - y_pred)\n    grad_beta_1 &lt;- mean((y - y_pred) * x)\n    beta_0 &lt;- beta_0 + learning_rate * grad_beta_0\n    beta_1 &lt;- beta_1 + learning_rate * grad_beta_1\n    store_beta_0[i] &lt;- beta_0\n    store_beta_1[i] &lt;- beta_1\n  }\n  data.frame(beta_1 = store_beta_1, beta_0 = store_beta_0)\n}\n\nWhat do you notice as difference? The sign!\n\nga_result &lt;- gradient_ascent(x, y, learning_rate = 0.01, iterations = 10)\nView(ga_result)\n\n\npaste0(\"Glucose/Slope:, \", ga_result$beta_1[dim(ga_result)[2]])\n\n[1] \"Glucose/Slope:, 0.381357711035091\"\n\npaste0(\"Intercept: \", ga_result$beta_0[dim(ga_result)[2]])\n\n[1] \"Intercept: 0.00194621080983146\"\n\n\nIf we fit the logistic regression model as before we get:\n\n# Fit a logistic regression model using glm\n\ndataset &lt;- data.frame(x =x, y = y)  # random dummy diabetes values\n\nglm_model &lt;- glm(y ~ x, data = dataset, family = binomial)\ncoef(glm_model)\n\n(Intercept)           x \n-5.35008039  0.03787304 \n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Supervised Learning:Logistic Regression",
      "3 Gradients"
    ]
  },
  {
    "objectID": "LogisticRegression.html",
    "href": "LogisticRegression.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "LogisticRegression.html#classes-powerpoint-presentation",
    "href": "LogisticRegression.html#classes-powerpoint-presentation",
    "title": "",
    "section": "Classes PowerPoint Presentation",
    "text": "Classes PowerPoint Presentation\n\n\n#| echo: false #| results: hide #| output: false #| include: false\n\nlibrary(tidyverse) \nlibrary(caret) \ntheme_set(theme_bw())\n\nLoad the data and remove NAs\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\nPimaIndiansDiabetes2 &lt;- na.omit(PimaIndiansDiabetes2)\n\nInspect the data\n\nsample_n(PimaIndiansDiabetes2, 3)\n\n    pregnant glucose pressure triceps insulin mass pedigree age diabetes\n748        1      81       74      41      57 46.3    1.096  32      neg\n611        3     106       54      21     158 30.9    0.292  24      neg\n727        1     116       78      29     180 36.1    0.496  25      neg\n\n\nSplit the data into training and test set\n\nset.seed(123)\ntraining.samples &lt;- PimaIndiansDiabetes2$diabetes %&gt;% \n  createDataPartition(p = 0.8, list = FALSE)\ntrain.data  &lt;- PimaIndiansDiabetes2[training.samples, ]\ntest.data &lt;- PimaIndiansDiabetes2[-training.samples, ]\n\nFit the model\n\nmodel &lt;- glm( diabetes ~., data = train.data, family = binomial)\n\nSummarize the model\n\nsummary(model)\n\n\nCall:\nglm(formula = diabetes ~ ., family = binomial, data = train.data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5832  -0.6544  -0.3292   0.6248   2.5968  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.053e+01  1.440e+00  -7.317 2.54e-13 ***\npregnant     1.005e-01  6.127e-02   1.640  0.10092    \nglucose      3.710e-02  6.486e-03   5.719 1.07e-08 ***\npressure    -3.876e-04  1.383e-02  -0.028  0.97764    \ntriceps      1.418e-02  1.998e-02   0.710  0.47800    \ninsulin      5.940e-04  1.508e-03   0.394  0.69371    \nmass         7.997e-02  3.180e-02   2.515  0.01190 *  \npedigree     1.329e+00  4.823e-01   2.756  0.00585 ** \nage          2.718e-02  2.020e-02   1.346  0.17840    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 398.80  on 313  degrees of freedom\nResidual deviance: 267.18  on 305  degrees of freedom\nAIC: 285.18\n\nNumber of Fisher Scoring iterations: 5\n\n\nMake predictions\n\nprobabilities &lt;- model %&gt;% predict(test.data, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\n\nModel accuracy\n\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7564103\n\nmodel &lt;- glm( diabetes ~ glucose, data = train.data, family = binomial)\nsummary(model)$coef\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -6.15882009 0.700096646 -8.797100 1.403974e-18\nglucose      0.04327234 0.005341133  8.101716 5.418949e-16\n\n\n\nnewdata &lt;- data.frame(glucose = c(20,  180))\nprobabilities &lt;- model %&gt;% predict(newdata, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\npredicted.classes\n\n    1     2 \n\"neg\" \"pos\" \n\ntrain.data %&gt;%\n  mutate(prob = ifelse(diabetes == \"pos\", 1, 0)) %&gt;%\n  ggplot(aes(glucose, prob)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) +\n  labs(\n    title = \"Logistic Regression Model\", \n    x = \"Plasma Glucose Concentration\",\n    y = \"Probability of being diabete-pos\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nmodel &lt;- glm( diabetes ~ glucose + mass + pregnant, \n              data = train.data, family = binomial)\nsummary(model)$coef\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -9.32369818 1.125997285 -8.280391 1.227711e-16\nglucose      0.03886154 0.005404219  7.190962 6.433636e-13\nmass         0.09458458 0.023529905  4.019760 5.825738e-05\npregnant     0.14466661 0.045125729  3.205857 1.346611e-03\n\nmodel &lt;- glm( diabetes ~., data = train.data, family = binomial)\nsummary(model)$coef\n\n                 Estimate  Std. Error     z value     Pr(&gt;|z|)\n(Intercept) -1.053400e+01 1.439679266 -7.31690975 2.537464e-13\npregnant     1.005031e-01 0.061266974  1.64041157 1.009196e-01\nglucose      3.709621e-02 0.006486093  5.71934633 1.069346e-08\npressure    -3.875933e-04 0.013826185 -0.02803328 9.776356e-01\ntriceps      1.417771e-02 0.019981885  0.70952823 4.779967e-01\ninsulin      5.939876e-04 0.001508231  0.39383055 6.937061e-01\nmass         7.997447e-02 0.031798907  2.51500698 1.190300e-02\npedigree     1.329149e+00 0.482291020  2.75590704 5.852963e-03\nage          2.718224e-02 0.020199295  1.34570257 1.783985e-01\n\ncoef(model)\n\n  (Intercept)      pregnant       glucose      pressure       triceps \n-1.053400e+01  1.005031e-01  3.709621e-02 -3.875933e-04  1.417771e-02 \n      insulin          mass      pedigree           age \n 5.939876e-04  7.997447e-02  1.329149e+00  2.718224e-02 \n\nmodel &lt;- glm( diabetes ~ pregnant + glucose + pressure + mass + pedigree, \n              data = train.data, family = binomial)\n\nprobabilities &lt;- model %&gt;% predict(test.data, type = \"response\")\nhead(probabilities)\n\n       19        21        32        55        64        71 \n0.1352603 0.5127526 0.6795376 0.7517408 0.2734867 0.1648174 \n\ncontrasts(test.data$diabetes)\n\n    pos\nneg   0\npos   1\n\npredicted.classes &lt;- ifelse(probabilities &gt; 0.5, \"pos\", \"neg\")\nhead(predicted.classes)\n\n   19    21    32    55    64    71 \n\"neg\" \"pos\" \"pos\" \"pos\" \"neg\" \"neg\" \n\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7564103\n\nlibrary(\"mgcv\")\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nThis is mgcv 1.8-42. For overview type 'help(\"mgcv-package\")'.\n\n# Fit the model\ngam.model &lt;- gam(diabetes ~ s(glucose) + mass + pregnant,\n                 data = train.data, family = \"binomial\")\n# Summarize model\nsummary(gam.model )\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\ndiabetes ~ s(glucose) + mass + pregnant\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -4.59794    0.86982  -5.286 1.25e-07 ***\nmass         0.09458    0.02353   4.020 5.83e-05 ***\npregnant     0.14467    0.04513   3.206  0.00135 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df Chi.sq p-value    \ns(glucose)   1      1  51.71  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.339   Deviance explained = 29.8%\nUBRE = -0.083171  Scale est. = 1         n = 314\n\n# Make predictions\nprobabilities &lt;- gam.model %&gt;% predict(test.data, type = \"response\")\npredicted.classes &lt;- ifelse(probabilities&gt; 0.5, \"pos\", \"neg\")\n# Model Accuracy\nmean(predicted.classes == test.data$diabetes)\n\n[1] 0.7820513"
  },
  {
    "objectID": "LogisticRegression.html#optional",
    "href": "LogisticRegression.html#optional",
    "title": "",
    "section": "optional ========================================================================",
    "text": "optional ========================================================================\nExample implementation from scratch:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nN &lt;- 200 # number of points per class\nD &lt;- 2 # dimensionality, we use 2D data for easy visualization\nK &lt;- 2 # number of classes, binary for logistic regression\nX &lt;- data.frame() # data matrix (each row = single example, can view as xy coordinates)\ny &lt;- data.frame() # class labels\n\nset.seed(56)\n\nt &lt;- seq(0,1,length.out = N) \nfor (j in (1:K)){\n  # t, m are parameters of parametric equations x1, x2\n  # t &lt;- seq(0,1,length.out = N) \n  # add randomness \n  m &lt;- rnorm(N, j+0.5, 0.25) \n  Xtemp &lt;- data.frame(x1 = 3*t , x2 = m - t) \n  ytemp &lt;- data.frame(matrix(j-1, N, 1))\n  X &lt;- rbind(X, Xtemp)\n  y &lt;- rbind(y, ytemp)\n}\n\ndata &lt;- cbind(X,y)\ncolnames(data) &lt;- c(colnames(X), 'label')\n\n# create dir images\n#dir.create(file.path('.', 'images'), showWarnings = FALSE)\n\n# lets visualize the data:\ndata_plot &lt;- ggplot(data) + geom_point(aes(x=x1, y=x2, color = as.character(label)), size = 2) + \n  scale_colour_discrete(name  =\"Label\") + \n  ylim(0, 3) + coord_fixed(ratio = 1) +\n  ggtitle('Data to be classified') +\n  theme_bw(base_size = 12) +\n  theme(legend.position=c(0.85, 0.87))\n\n#png(file.path('images', 'data_plot.png'))\nprint(data_plot)\n\n\n\n#dev.off()\n\n#sigmoid function, inverse of logit\nsigmoid &lt;- function(z){1/(1+exp(-z))}\n\n#cost function\ncost &lt;- function(theta, X, y){\n  m &lt;- length(y) # number of training examples\n  h &lt;- sigmoid(X %*% theta)\n  J &lt;- (t(-y)%*%log(h)-t(1-y)%*%log(1-h))/m\n  J\n}\n\n#gradient function\ngrad &lt;- function(theta, X, y){\n  m &lt;- length(y) \n  \n  h &lt;- sigmoid(X%*%theta)\n  grad &lt;- (t(X)%*%(h - y))/m\n  grad\n}\n\nlogisticReg &lt;- function(X, y){\n  #remove NA rows\n  X &lt;- na.omit(X)\n  y &lt;- na.omit(y)\n  #add bias term and convert to matrix\n  X &lt;- mutate(X, bias =1)\n  #move the bias column to col1\n  X &lt;- as.matrix(X[, c(ncol(X), 1:(ncol(X)-1))])\n  y &lt;- as.matrix(y)\n  #initialize theta\n  theta &lt;- matrix(rep(0, ncol(X)), nrow = ncol(X))\n  #use the optim function to perform gradient descent\n  costOpti &lt;- optim(theta, fn = cost, gr = grad, X=X, y=y)\n  #return coefficients\n  return(costOpti$par)\n}\n\nlogisticProb &lt;- function(theta, X){\n  X &lt;- na.omit(X)\n  #add bias term and convert to matrix\n  X &lt;- mutate(X, bias =1)\n  X &lt;- as.matrix(X[,c(ncol(X), 1:(ncol(X)-1))])\n  return(sigmoid(X%*%theta))\n}\n\nlogisticPred &lt;- function(prob){\n  return(round(prob, 0))\n}\n\n# training\ntheta &lt;- logisticReg(X, y)\nprob &lt;- logisticProb(theta, X)\npred &lt;- logisticPred(prob)\n\n# generate a grid for decision boundary, this is the test set\ngrid &lt;- expand.grid(seq(0, 3, length.out = 100), seq(0, 3, length.out = 100))\n# predict the probability\nprobZ &lt;- logisticProb(theta, grid)\n# predict the label\nZ &lt;- logisticPred(probZ)\ngridPred = cbind(grid, Z)\n\n# decision boundary visualization\np &lt;- ggplot() + geom_point(data = data, aes(x=x1, y=x2, color = as.character(label)), size = 2, show.legend = F) + \n  geom_tile(data = gridPred, aes(x = grid[, 1],y = grid[, 2], fill=as.character(Z)), alpha = 0.3, show.legend = F)+ \n  ylim(0, 3) +\n  ggtitle('Decision Boundary for Logistic Regression') +\n  coord_fixed(ratio = 1) +\n  theme_bw(base_size = 12) \n\n#png(file.path('images', 'logistic_regression.png'))\nprint(p)\n\nWarning: Removed 200 rows containing missing values (`geom_tile()`).\n\n\n\n\n#dev.off()"
  }
]